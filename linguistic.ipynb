{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/padariya-tech/Opencv_codes/blob/main/linguistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHnbyVzqiftH"
      },
      "source": [
        "# **Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTpJ73NvVhuM",
        "outputId": "1542635c-59f9-4d11-eff7-5362db601dae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-c9242a88dfc1>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  data = pd.read_csv('/content/drive/MyDrive/gu_100.txt',on_bad_lines='skip', sep='delimiter',header = None, encoding = 'utf-8')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                   0\n",
            "0  ૯મી ઓગસ્ટ ૨૦૧૬ના રોજ આદિવાસી વિકાસ સંગઠન દ્વાર...\n",
            "1  આ પતાવટની આંતરમાળખા ખૂબ સારી રીતે વિકસિત નથી, ...\n",
            "2  વહીવટ બિલ્ડિંગ નજીક પાછળના બાજુ પર, હોટેલ આંતર...\n",
            "3  ગુરુવારે સવારે બેંકો ખુલતા પહેલા પ્રતિબંધિત નો...\n",
            "4  ઈન્ડિયન આઈડલ 11ના આગામી એપિસોડમાં ઉદિત નારાયણ ...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_csv('/content/drive/MyDrive/gu_100.txt',on_bad_lines='skip', sep='delimiter',header = None, encoding = 'utf-8')\n",
        "#importing dataframe\n",
        "\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI3WyiJQikX4"
      },
      "source": [
        "**https://learngujaratiwithme.com/gujarati-vowels/ getting list from this website**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3frmWd6GHgN"
      },
      "outputs": [],
      "source": [
        "swar = ['અ', 'આ', 'ઇ', 'ઈ', 'ઉ', 'ઊ', 'ઋ', 'ઌ', 'ઍ', 'એ', 'ઐ', 'ઑ', 'ઓ', 'ઔ','અં','અઃ']\n",
        "matra = ['ા', 'િ', 'ી', 'ુ', 'ૂ', 'ૃ', 'ૄ', 'ૅ', 'ે', 'ૈ', 'ૉ', 'ો', 'ૌ']\n",
        "vyanjan = ['ક', 'ખ', 'ગ', 'ઘ', 'ઙ', 'ચ', 'છ', 'જ', 'ઝ', 'ઞ', 'ટ', 'ઠ', 'ડ', 'ઢ', 'ણ', 'ત', 'થ', 'દ', 'ધ', 'ન', 'પ', 'ફ', 'બ', 'ભ', 'મ', 'ય', 'ર', 'લ', 'ળ', 'વ', 'શ', 'ષ', 'સ', 'હ']\n",
        "gujarati_dict = {\n",
        "    'ા': 'આ',\n",
        "    'િ': 'ઇ',\n",
        "    'ી': 'ઈ',\n",
        "    'ુ': 'ઉ',\n",
        "    'ૂ': 'ઊ',\n",
        "    'ૃ': 'ઋ',\n",
        "    'ૄ': 'ઌ',\n",
        "    'ૅ': 'ઍ',\n",
        "    'ે': 'એ',\n",
        "    'ૈ': 'ઐ',\n",
        "    'ૉ': 'ઑ',\n",
        "    'ો': 'ઓ',\n",
        "    'ૌ': 'ઔ',\n",
        "    'ં':'અં',\n",
        "    'ઃ':'અઃ'\n",
        "}\n",
        "\n",
        "word = \"માત્ર\"\n",
        "punctuation_list = [\n",
        "'>्','0्', '1्', '\"्', 'ळ्', '2्', '–्', 'ः्', '3्', '5्', '4्', '््', '%्', '—्', '8्', '6्', '7्', '9्', 'ॅ्', 'a्', '>्', 'e्', '#्', 'i्','r्', 't्', '»्', 'o्', 'n्', 'd्', '०्', 's्', 'h्', 'l्', 'c्', 'm्',\n",
        "'\\u200c्', '\\u200b्', 'ï्', 'A्', 'p्', '•्', 'b्', 'G्', 'B्', '&्', 'u्',\n",
        "'_्', '@्', 'M्', 'о्', 'f्', '·्', '$्', 'S्', 'g्', 'I्', 'а्', 'е्', 'P्',\n",
        "'и्', 'R्', 'y्', 'k्', 'w्', 'T्', '�्', 'a','b','c','d','e','f','g','h','i','j',\n",
        " 'k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','0','1','2','3','4','5','6','7','8','9',\n",
        " 'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z',\n",
        " '!','\"','\\\\','#','\\\\','$','%','\\\\','&',\"'\",'\\\\','(','\\\\',')','\\\\','*','\\\\','+',',','\\\\','-','\\\\','.','/',\n",
        " ':',';','<','=','>','\\\\','?','@','\\\\','[','\\\\','\\\\','\\\\',']','\\\\','^','_','`','\\\\','{','\\\\','|','\\\\','}','\\\\','~']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmb1OXL9ivTq"
      },
      "source": [
        "# **Q1 Perform the Unicode correction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omgqajwTjEgw"
      },
      "source": [
        "**wrote function for unicode correction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpZUtdm5-R8H"
      },
      "outputs": [],
      "source": [
        "\n",
        "def correct_unicode(word):\n",
        "  correct_word=\"\"\n",
        "  length = len(word)\n",
        "  for i in range(length):\n",
        "\n",
        "        if word[i] in vyanjan:\n",
        "          correct_word +=' '+word[i]+\"્\"\n",
        "          if i<=length-2 and (word[i+1]=='्' or word[i+1] in punctuation_list):\n",
        "            pass\n",
        "          elif i<=length-2 and word[i+1] in matra:\n",
        "            correct_word +=' ' +gujarati_dict[word[i+1]]\n",
        "          elif i<=length-2 and (word[i+1] in vyanjan or word[i+1] in swar) :\n",
        "            correct_word +=' ' +'અ'\n",
        "          elif i == length - 1:\n",
        "                correct_word +=' ' +'અ'\n",
        "        elif word[i] in swar :\n",
        "          correct_word +=' ' +word[i]\n",
        "\n",
        "\n",
        "  return correct_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlsBPfyjju5K"
      },
      "source": [
        "**print for first 10 row from data file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s8bd2-SR9Pw"
      },
      "outputs": [],
      "source": [
        "counter = 0\n",
        "processed_words = [] # list contain all the corrected unicode\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    # Check if we have processed 10 rows\n",
        "    # if counter >= 10 :\n",
        "        # break\n",
        "\n",
        "    # Get the text data from the current row\n",
        "    text = row[0]\n",
        "\n",
        "    # Split the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Process each word and store cleaned versions\n",
        "    for word in words:\n",
        "        cleaned_word = correct_unicode(word)  # Explain what this function does\n",
        "        processed_words.append(cleaned_word)\n",
        "        # print(f\"{word} => {cleaned_word}\")\n",
        "\n",
        "    # Increment the counter\n",
        "    # counter += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BJeaf6peCYO",
        "outputId": "3e9af0dc-16d0-4392-f074-141edd0fe1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' મ્ ઈ', ' ઓ ગ્ અ સ્ ટ્ અ', ' ન્ આ', ' ર્ ઓ જ્ અ', ' આ દ્ ઇ વ્ આ સ્ ઈ', ' વ્ ઇ ક્ આ સ્ અ', ' સ્ ગ્ અ ઠ્ અ ન્ અ', ' દ્ વ્ આ ર્ આ', ' આ દ્ ઇ વ્ આ સ્ ઈ', ' ભ્ અ વ્ અ ન્ અ', ' ખ્ આ ત્ એ', ' ખ્ ઊ બ્ અ', ' જ્ અ', ' ઉ ત્ સ્ આ હ્ અ ભ્ એ ર્ અ', ' ઉ જ્ અ વ્ અ ણ્ ઈ', ' ક્ અ ર્ અ વ્ આ મ્ આ', ' આ વ્ અ શ્ એ', ' આ', ' પ્ અ ત્ આ વ્ અ ટ્ અ ન્ ઈ', ' આ ત્ અ ર્ અ મ્ આ ળ્ અ ખ્ આ', ' ખ્ ઊ બ્ અ', ' સ્ આ ર્ ઈ', ' ર્ ઈ ત્ એ', ' વ્ ઇ ક્ અ સ્ ઇ ત્ અ', ' ન્ અ થ્ ઈ', ' પ્ અ ર્ ત્ ઉ', ' પ્ ર્ અ વ્ આ સ્ ઈ ઓ', ' અ ન્ એ', ' ઉ દ્ ય્ ઓ ગ્ અ પ્ અ ત્ ઇ ઓ ન્ ઈ', ' ડ્ અ ઝ્ અ ન્ એ ક્ અ', ' અ હ્ ઈ', ' બ્ અ ધ્ આ', ' જ્ અ', ' આ વ્ એ', ' છ્ એ', ' વ્ અ હ્ ઈ વ્ અ ટ્ અ', ' બ્ ઇ લ્ ડ્ ઇ ગ્ અ', ' ન્ અ જ્ ઈ ક્ અ', ' પ્ આ છ્ અ ળ્ અ ન્ આ', ' બ્ આ જ્ ઉ', ' પ્ અ ર્', ' હ્ ઓ ટ્ એ લ્ અ', ' આ ત્ અ ર્ ઇ ક્ અ', ' વ્ ઇ સ્ ત્ આ ર્', ' ત્ એ ન્ આ', ' અ ત્ અ', ' બ્ એ', ' થ્ ઈ', ' ચ્ આ ર્ અ', ' મ્ આ ળ્ અ ન્ ઉ', ' ર્ અ હ્ એ ણ્ આ ક્ અ', ' ઇ મ્ આ ર્ અ ત્ ઓ', ' અ ડ્ ઈ ન્ એ', ' મ્ અ ર્ ય્ આ દ્ ઇ ત્ અ', ' ક્ અ ર્ એ', ' છ્ એ', ' ગ્ ઉ ર્ ઉ વ્ આ ર્ એ', ' સ્ અ વ્ આ ર્ એ', ' બ્ એ ક્ ઓ', ' ખ્ ઉ લ્ અ ત્ આ', ' પ્ અ હ્ એ લ્ આ', ' પ્ ર્ અ ત્ ઇ બ્ ધ્ ઇ ત્ અ', ' ન્ ઓ ટ્ ઓ', ' લ્ અ ઇ ન્ એ', ' મ્ ઓ ટ્ ઈ', ' સ્ ખ્ ય્ આ મ્ આ', ' લ્ ઓ ક્ ઓ', ' બ્ એ ક્ ઓ ન્ ઈ', ' બ્ અ હ્ આ ર્ અ', ' ઊ ભ્ આ', ' ર્ અ હ્ ઈ', ' ગ્ અ ય્ આ', ' હ્ અ ત્ આ', ' ઈ ન્ ડ્ ઇ ય્ અ ન્ અ', ' આ ઈ ડ્ અ લ્ અ', ' ન્ આ', ' આ ગ્ આ મ્ ઈ', ' એ પ્ ઇ સ્ ઓ ડ્ અ મ્ આ', ' ઉ દ્ ઇ ત્ અ', ' ન્ આ ર્ આ ય્ અ ણ્ અ', ' અ ન્ એ', ' અ લ્ અ ક્ આ', ' ય્ આ જ્ ઞ્ ઇ ક્ અ', ' મ્ અ હ્ એ મ્ આ ન્ અ', ' બ્ અ ન્ અ શ્ એ', ' ત્ એ', ' બ્ આ જ્ ઉ ન્ ઈ', ' મ્ એ ન્ સ્ ઇ સ્ સ્ અ', ' સ્ આ થ્ એ', ' ત્ અ મ્ આ ર્ આ', ' ઘ્ ઊ ટ્ અ ણ્ અ ન્ ઈ', ' સ્ ય્ ઉ ક્ ત્ અ', ' દ્ વ્ આ ર્ આ', ' શ્ ઓ ક્ અ', ' શ્ ઓ ષ્ અ ણ્ અ', ' મ્ આ ટ્ એ', ' જ્ અ વ્ આ બ્ અ દ્ આ ર્ અ', ' છ્ એ', ' દ્ એ શ્ અ ન્ ઈ', ' ઉ ચ્ ચ્ અ']\n"
          ]
        }
      ],
      "source": [
        "N = 100\n",
        "result=[]\n",
        "result = processed_words[:N]\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2GKqR6hWAmo"
      },
      "source": [
        "# **Q2 Find all characters and syllables. Store a list of them in descending order of their frequencies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM6wpghIhq4T"
      },
      "source": [
        "**uni-gram and bi-gram frequencies of Character**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR6WzHfoTlQe"
      },
      "outputs": [],
      "source": [
        "# craeted global list to store frequency for character\n",
        "unigram_char_list=[]\n",
        "\n",
        "unigram_char_list_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def unigram_for_char(each_word, vyanjan_list):\n",
        "  # Join the strings and remove spaces\n",
        "  joined_string = ''.join(each_word).replace(\" \", \"\")\n",
        "  # print(joined_string)\n",
        "  # Count character frequencies, treating '્' as part of the preceding character\n",
        "  char_counts = Counter(\n",
        "      c for c in joined_string if c != \"્\" or c == joined_string[joined_string.index(c) - 1]\n",
        "  )\n",
        "\n",
        "  # Sort characters by frequency (descending)\n",
        "  sorted_chars = sorted(char_counts, key=char_counts.get, reverse=True)\n",
        "\n",
        "  # Create a list of characters with halant (if needed)\n",
        "  # char_freq_list = [\n",
        "  #     (char + \"્\" if char in vyanjan_list else char, char_counts[char])\n",
        "  #     for char in sorted_chars\n",
        "  # ]\n",
        "\n",
        "  for char, freq in char_counts.items():\n",
        "        char_key = char + \"્\" if char in vyanjan_list else char\n",
        "        found = False\n",
        "        for i, (existing_char, existing_freq) in enumerate(unigram_char_list):\n",
        "            if existing_char == char_key:\n",
        "                unigram_char_list[i] = (existing_char, existing_freq + freq)\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            unigram_char_list.append((char_key, freq))\n",
        "\n",
        "def sort_list_by_frequency(char_freq_list):\n",
        "    return sorted(char_freq_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "for word in processed_words:\n",
        "\n",
        "  unigram_for_char(word, vyanjan) # unigram for character\n",
        "\n",
        "  # print(\" unigram characters \")\n",
        "\n",
        "unigram_char_list_sorted=sort_list_by_frequency(unigram_char_list)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbxJT60oittx",
        "outputId": "2ba975e0-c65e-4e2e-c50b-68debec6693f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('અ', 6781001), ('આ', 3692279), ('એ', 2522856), ('ર્', 2128526), ('ન્', 1734112), ('ઈ', 1589073), ('ક્', 1386555), ('મ્', 1324843), ('ઓ', 1236360), ('ત્', 1207724), ('વ્', 1149790), ('સ્', 976491), ('પ્', 867048), ('ઇ', 838885), ('ય્', 745130), ('લ્', 684911), ('ઉ', 673954), ('જ્', 585042), ('ટ્', 520557), ('હ્', 516880)]\n"
          ]
        }
      ],
      "source": [
        "print(unigram_char_list_sorted[:20])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WSzwspIav4wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_char_list=[]\n",
        "bigram_char_list_sorted=[]\n",
        "def sort_list_by_frequency(char_freq_list):\n",
        "    return sorted(char_freq_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "def bigram_for_char(each_word, vyanjan_list):\n",
        "\n",
        "    bigram_counts = {}\n",
        "    joined_string = ''.join(each_word).replace(\" \", \"\")\n",
        "\n",
        "    for i in range(len(joined_string) - 1):\n",
        "        char1 = joined_string[i]\n",
        "        char2 = joined_string[i + 1]\n",
        "\n",
        "        if char1 == \"્\":\n",
        "            continue\n",
        "\n",
        "        elif char2 == \"્\":\n",
        "            if i + 2 < len(joined_string) and joined_string[i + 2] != \"્\":\n",
        "                bigram = char1 + joined_string[i + 1] + joined_string[i + 2]\n",
        "                i += 1\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        else:\n",
        "            bigram = char1 + char2\n",
        "\n",
        "        bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "\n",
        "    bigram_list = [(bigram, count) for bigram, count in bigram_counts.items()]\n",
        "\n",
        "\n",
        "    for bigram, count in bigram_list:\n",
        "        found = False\n",
        "        for i, (existing_bigram, existing_count) in enumerate(bigram_char_list):\n",
        "            if existing_bigram == bigram:\n",
        "                bigram_char_list[i] = (existing_bigram, existing_count + count)\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            bigram_char_list.append((bigram, count))\n",
        "\n",
        "    bigram_list = [(bigram, count) for bigram, count in bigram_counts.items()]\n",
        "    # bigram_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return bigram_list\n",
        "for word in processed_words:\n",
        "   bigram_for_char(word, vyanjan) # bigram for character\n",
        "bigram_char_list_sorted=sort_list_by_frequency(bigram_char_list)"
      ],
      "metadata": {
        "id": "EeRE5MPxlzS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bigram_char_list_sorted[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wimm_xpUn5Du",
        "outputId": "edf197f7-21a2-4852-fbc4-940313a844e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ર્અ', 900967), ('અર', 856288), ('અન', 740690), ('ક્અ', 615761), ('મ્આ', 578634), ('આર', 436837), ('પ્અ', 412888), ('અમ', 405502), ('વ્આ', 403416), ('મ્અ', 393171), ('ન્અ', 374634), ('ન્એ', 374190), ('અત', 357387), ('અવ', 352507), ('સ્અ', 343310), ('ત્અ', 339485), ('ન્આ', 338880), ('છ્એ', 322605), ('વ્અ', 319748), ('ય્અ', 275049)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWaHqZpM3JuI"
      },
      "source": [
        "**uni-gram and bi-gram frequencies of syllables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfSnc31S3Zbs"
      },
      "outputs": [],
      "source": [
        "unigram_syllables_list=[]\n",
        "unigram_syllabless=[]\n",
        "unigram_syllables_list_sorted=[]\n",
        "### unigram syllables ####################################################################################################################\n",
        "def remove_last_two(string):\n",
        "\n",
        "  if not string:\n",
        "    return \"\"\n",
        "  return string[:-1]\n",
        "def unigram_syllables(text,swar,vyanjan,swapped_dict):\n",
        "\n",
        "    i = 0\n",
        "    syllables_counts = {}\n",
        "    text = ''.join(text).replace(\" \", \"\")\n",
        "    syllable=[]\n",
        "    while i < len(text):\n",
        "          ch=\"\"\n",
        "          while i < len(text) and text[i] not in swar:\n",
        "\n",
        "              ch += text[i]\n",
        "\n",
        "              i+=1\n",
        "          if i < len(text) and len(ch)>1 and text[i] == 'અ':\n",
        "\n",
        "            ch=remove_last_two(ch)\n",
        "          elif i < len(text) and len(ch)>1 :\n",
        "            ch=remove_last_two(ch)\n",
        "            ch+=swapped_dict[text[i]]\n",
        "          elif i < len(text) :\n",
        "            ch = ch+text[i]\n",
        "          syllables_counts[ch] = syllables_counts.get(ch, 0) + 1\n",
        "          syllable.append(ch)\n",
        "          i+=1\n",
        "\n",
        "    syllable_list = [(syllable, count) for syllable, count in syllables_counts.items()]\n",
        "\n",
        "    syllable_list.sort(key=lambda x: x[1], reverse=True)\n",
        "    for syllable, count in syllable_list:\n",
        "        found = False\n",
        "        for i, (existing_syllable, existing_count) in enumerate(unigram_syllables_list):\n",
        "            if existing_syllable == syllable:\n",
        "                unigram_syllables_list[i] = (existing_syllable, existing_count + count)\n",
        "                unigram_syllabless.append(existing_syllable)\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            unigram_syllables_list.append((syllable, count))\n",
        "            unigram_syllabless.append(syllable)\n",
        "\n",
        "    # print(\" unigram syllables \")\n",
        "    # print( syllable_list)\n",
        "    # return syllable\n",
        "\n",
        "\n",
        "# Example usage\n",
        "\n",
        "swapped_dict = {v: k for k, v in gujarati_dict.items()}\n",
        "\n",
        "# print(swapped_dict)\n",
        "resullt = [' મ્ ઈ', ' ઓ ગ્ અ સ્ ટ્ અ', ' ન્ આ']\n",
        "spchar = '@'\n",
        "count =0\n",
        "for word in processed_words:\n",
        "\n",
        "  unigram_syllables(word,swar,vyanjan,swapped_dict)\n",
        "  unigram_syllabless.append(spchar)\n",
        "\n",
        "\n",
        "\n",
        "unigram_syllables_list_sorted=sort_list_by_frequency(unigram_syllables_list)\n",
        "# print(unigram_syllabless)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_syllables_list_sorted[:40])\n",
        "# print(unigram_syllables_list[:20])\n",
        "print(unigram_syllabless[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeWdjw4_ti-k",
        "outputId": "8d7a9e52-4fa4-413c-f134-6b2e7514c63c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ર', 737913), ('ક', 591539), ('મા', 567925), ('પ', 385962), ('વા', 370332), ('ને', 366840), ('મ', 366628), ('ન', 361478), ('અ', 353767), ('ના', 334899), ('આ', 330561), ('છે', 322018), ('ત', 299084), ('સ', 289024), ('વ', 284555), ('એ', 250054), ('ની', 240035), ('જ', 220568), ('તે', 213475), ('લ', 211768), ('રી', 208623), ('કા', 188375), ('હ', 186288), ('રા', 185782), ('ય', 176702), ('ગ', 174232), ('કે', 174127), ('તા', 165197), ('શ', 147717), ('સા', 146033), ('ણ', 145540), ('બ', 143713), ('દ', 143421), ('ટ', 140932), ('ઓ', 135868), ('થી', 133064), ('લા', 128953), ('રે', 127510), ('ઈ', 120587), ('કો', 116367)]\n",
            "['મી', '@', 'ઓ', 'ગ', 'સ્ટ', '@', 'ના', '@', 'રો', 'જ', '@', 'આ', 'દિ', 'વા', 'સી', '@', 'વિ', 'કા', 'સ', '@']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram_syllables_list=[]\n",
        "# bigram_syllables_list_sorted=[]\n",
        "# def bigram_syllables(syllable_list,swar,vyanjan):\n",
        "\n",
        "#       bigram_syllables_counts = {}\n",
        "#       i=1\n",
        "#       while i < len(syllable_list):\n",
        "#         ch = syllable_list[i-1]+syllable_list[i]\n",
        "#         bigram_syllables_counts[ch] = bigram_syllables_counts.get(ch, 0) + 1\n",
        "#         bigram_syllables_list.append(ch)\n",
        "\n",
        "#         i=i+1\n",
        "#       syllable_list = [(syllable, count) for syllable, count in bigram_syllables_counts.items()]\n",
        "#       syllable_list.sort(key=lambda x: x[1], reverse=True)\n",
        "#       return syllable_list\n",
        "\n",
        "\n",
        "# syllables2=bigram_syllables(unigram_syllables_list[:20],swar,vyanjan)\n",
        "# print(\" biagram syllables \")\n",
        "# print(syllables2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNk5clfAmQbA",
        "outputId": "81b1a532-2e9e-4699-f342-2ffe0b20f7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " biagram syllables \n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_syllables_list=[]\n",
        "bigram_syllables_list_sorted=[]\n",
        "def get_bisyllable_frequency(list_of_syllables):\n",
        "    biagram_syllables_list = []\n",
        "    for i in range(len(list_of_syllables) - 1):\n",
        "        if list_of_syllables[i+1] == \"@\":\n",
        "            pass\n",
        "        elif list_of_syllables[i] != \"@\":\n",
        "            pair = list_of_syllables[i] + \" \" + list_of_syllables[i+1]\n",
        "            biagram_syllables_list.append(pair)\n",
        "\n",
        "    bisyllable_frequency = {}\n",
        "    i = 0\n",
        "    while i < len(biagram_syllables_list):\n",
        "        ch = biagram_syllables_list[i]\n",
        "        bisyllable_frequency[ch] = bisyllable_frequency.get(ch, 0) + 1\n",
        "        i += 1\n",
        "\n",
        "    sort_bisyllable_list = [(bisyllable, count) for bisyllable, count in bisyllable_frequency.items()]\n",
        "    sort_bisyllable_list.sort(key=lambda x: x[1], reverse=True)\n",
        "    return sort_bisyllable_list\n",
        "bigram_syllables_list_sorted=get_bisyllable_frequency(unigram_syllabless)\n",
        "# print(bigram_syllables_list_sorted)\n"
      ],
      "metadata": {
        "id": "BMSicu5R8Fcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bigram_syllables_list_sorted[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJiNWJfp-U_w",
        "outputId": "786a6691-8182-43ac-d584-78602312a0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('અ ને', 156342), ('ક ર', 95306), ('મા ટે', 83961), ('ર વા', 68258), ('એ ક', 67765), ('પ ર', 64366), ('પ ણ', 58280), ('ક રી', 56269), ('વા મા', 55646), ('સા થે', 45581), ('તે મ', 42669), ('ત મા', 40013), ('કા ર', 38246), ('સ મ', 33326), ('હ તી', 32939), ('ન થી', 32321), ('ઉ પ', 29516), ('આ વે', 28206), ('ત મે', 26162), ('વ વા', 24444)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoodWj3oFbB5"
      },
      "source": [
        "# **Q3 Unigram, BPE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAzFzPflFuuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7029efd6-30d5-4d25-ffae-521e872b0bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BPE IMPLEMENT vocab size 1K**"
      ],
      "metadata": {
        "id": "rZn90DrGtSdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Train SentencePiece model\n",
        "spm.SentencePieceTrainer.train('--input=temp.txt --model_prefix=m_bpe --vocab_size=1000 --model_type=bpe')\n",
        "\n",
        "# Load the trained model\n",
        "sp_bpe = spm.SentencePieceProcessor()\n",
        "sp_bpe.load('m_bpe.model')\n",
        "\n",
        "# Tokenize text from 'temp.txt' file\n",
        "print('*** BPE ***')\n",
        "with open('temp.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "with open('ques_25.txt', 'r', encoding='utf-8') as file:\n",
        "    text2 = file.read()\n",
        "\n",
        "ques_25_bpe=sp_bpe.encode_as_pieces(text2)\n",
        "bpe_list=sp_bpe.encode_as_pieces(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl-l7Hvco7Kw",
        "outputId": "fd5405c6-50b2-43fa-e8a1-513fec7f1dd6"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** BPE ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "tokens_cleaned = [token.replace('▁', '') for token in bpe_list if token not in punctuation_list ]\n",
        "ques_25_cleaned_bpe = [token.replace('▁', '') for token in ques_25_bpe if token not in punctuation_list ]\n",
        "\n",
        "# Count the frequency of each token\n",
        "token_freq = Counter(tokens_cleaned)\n",
        "\n",
        "bpe_token_freq_list = list(token_freq.items())\n",
        "# print(token_freq_list)\n",
        "bpe_sorted_token_freq_list = sorted(bpe_token_freq_list, key=lambda x: x[1], reverse=True)\n",
        "print(bpe_sorted_token_freq_list[:20])\n",
        "print(ques_25_cleaned_bpe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjXk8xGuqg8a",
        "outputId": "97b35b12-5dcd-4244-803d-1d8115eb556f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('છે', 44), ('માં', 23), ('અને', 16), ('આ', 15), ('પ', 14), ('જ', 13), ('સ', 13), ('ની', 12), ('ને', 12), ('હ', 11), ('એ', 11), ('ક', 11), ('માટે', 11), ('વ', 10), ('તે', 10), ('ન', 10), ('ત', 10), ('ના', 9), ('સાથે', 9), ('જે', 9)]\n",
            "['ડ', 'િસ્', 'પ્', 'લે', 'પર', 'પણ', 'યા', 'ંત', '્ર', 'િક', 'ન', 'ુક', 'સાન', 'કો', 'ર્', 'નિંગ', 'ગ્લાસ', 'થી', 'એક', 'ખા', 'સ', 'ર', 'ક્ષ', 'ણા', 'ત્મક', 'સ્', 'તર', 'છે', '', '', 'ઇ', '1', '70', 'પા', 'ક', 'િસ્', 'તા', 'ન', 'ના', 'પીએ', 'મ', 'ઈ', 'મ', 'રા', 'ન', 'ખ', 'ાન', 'ની', 'અ', 'ધ', '્ય', 'ક્ષ', 'તા', 'માં', 'આ', 'બે', 'ઠ', 'ક', 'બ', 'ો', 'લા', 'વવા', 'માં', 'આવી', 'છે', 'આવી', 'ઘ', 'ટના', 'માં', 'છે', 'તરપિ', 'ંડી', 'કર', 'નાર', 'વ', '્ય', 'ક્તિ', 'મો', 'ટે', 'ભા', 'ગે', 'પરિ', 'ચિત', 'અથવા', 'તો', 'સં', 'બંધી', 'હોય', 'છે', 'એ', 'વા', 'ત', 'વધ', 'ારે', 'આ', 'ઘાત', 'જનક', 'હોય', 'છે', 'આ', 'પ', 'તાવટ', 'ની', 'આંતર', 'માળ', 'ખા', 'ખૂબ', 'સાર', 'ી', 'રીતે', 'વ', 'િક', 'સિત', 'નથી', 'આ', 'ર', 'ક', 'મ', 'આ', 'શ', 'ર', 'ે', 'છે', 'અ', 'મ', 'ે', 'આ', 'મા', 'મ', 'લે', 'બહ', 'ાર', 'નાં', 'વ', '્ય', 'ક્તિ', 'નાં', 'કોઇ', 'પણ', 'દ', 'ા', 'વા', 'ને', 'સં', 'પ', 'ૂ', 'ર્', 'ણ', 'રીતે', 'ફ', 'ગ', 'ા', 'વી', 'એ', 'છ', 'ી', 'એ', ')', '-', 'આ', 'ક', 'િસ્', 'સા', 'ઓ', 'માં', 'એન્', 'ટ', 'િ', 'બ', 'ાય', 'ો', 'ટ', 'િ', 'ક્', 'સ', 'સાથે', 'સારવાર', 'જ', 'રૂ', 'ર', 'છે', 'તમારી', 'ર', 'ા', 'શ', 'િ', 'ના', 'લોકો', 'એ', 'હ', 'ાલ', 'દ', 'ુ', 'ર્', 'ઘ', 'ટના', 'ઓ', 'થી', 'સા', 'વ', 'ધાન', 'રહે', 'વાની', 'જ', 'રૂ', 'ર', 'છે', 'આ', 'બાબત', 'છે', 'ત', '્ય', 'ાર', 'બ', 'ાદ', 'સ', 'ી', 'બી', 'આ', 'ઇ', 'અને', 'ઇ', 'ડી', 'એ', 'પૂ', 'રક', 'ચાર', '્', 'જ', 'શ', 'ી', 'ટ', 'દ', 'ાખ', 'લ', 'કરી', 'હ', 'તી', '', 'ઈ', 'ન્ડ', 'િયન', 'આ', 'ઈડલ', '11', 'ના', 'આગામી', 'એ', 'પિસ', 'ોડ', 'માં', 'ઉ', 'દિત', 'ન', 'ાર', 'ાયણ', 'અને', 'અ', 'લકા', 'યા', 'જ્ઞ', 'િક', 'મહે', 'માન', 'બનશે', 'જ્યારે', 'ક', '્લે', 'રિસ', 'અને', 'મિ', 'ત્સુ', 'ઇનો', 'સ', 'માન', 'હ', 'િસ્સ', 'ો', 'હતો', 'આથી', 'ઋ', 'ષિક', 'ેશ', 'ન', 'ગર', 'માં', 'વિ', 'રભ', 'દ્ર', 'બંધના', 'દ્', 'વાર', 'પાસે', 'તે', 'ફ', 'સાઈ', 'ગયો', 'હતો', 'ઉ', 'રી', 'હ', 'ુ', 'મલા', 'પછી', 'ભ', 'ાર', 'તી', 'ય', 'સ', 'ે', 'ના', 'ત', 'ર', 'ફ', 'થી', 'સ', 'ર્', 'જ', 'િક', 'લ', 'સ્', 'ટ્ર', 'ાઇ', 'ક', 'કરવામાં', 'આવી', 'આ', 'ખૂબ', 'જ', 'ખ', 'તર', 'ના', 'ક', 'મિ', 'શ', 'નને', 'સ', 'ફ', 'ળ', 'તા', 'પૂ', 'ર્વ', 'ક', 'પ', 'ાર', 'પા', 'ડ', 'વા', 'નો', 'શ્ર', 'ે', 'ય', 'જ', 'ાય', 'છે', 'ભ', 'ાર', 'ત', 'ના', 'ર', 'ા', 'ષ્ટ્ર', 'ી', 'ય', 'સુર', 'ક્ષ', 'ા', 'સ', 'લા', 'હ', 'કાર', 'અ', 'જી', 'ત', 'ડ', 'ો', 'વા', 'લ', 'વ', '્યા', 'પ', 'ક', 'દર', 'િયા', 'ક', 'િ', 'નાર', 'ો', 'જે', 'ના', 'માટે', 'સરકાર', 'ે', 'ઘણી', 'બ', 'ધ', 'ી', '', 'યો', 'જ', 'ના', 'ઓ', 'પણ', 'ઘ', 'ડી', 'છે', 'કે', 'ખ', 'ે', 'ડ', 'ૂ', 'તો', 'ને', 'થ', 'ોડા', 'ઘણા', 'અ', 'ં', 'શે', 'પ', 'ોતા', 'ની', 'ત', 'ક', 'લી', 'ફ', 'માં', 'ર', 'ા', 'હ', 'ત', 'મળી', 'રહે', 'સ્', 'પ', 'ે', 'ન્સ', 'રી', 'સ', 'શ', 'િ', 'ષ', '્ય', 'વૃ', 'ત્તિ', 'જો', 'તમે', 'સ', 'લ્', 'ફ', 'ર', 'માંથી', 'સા', 'મ', 'ય', 'િક', '', 'ટે', 'બલ', 'ને', '', 'ની', 'ચ', 'ે', 'ખસે', 'ડ', 'ો', 'તેમણે', 'કર', 'ોડો', 'પતિ', 'નો', 'દર', 'જ્જો', 'પ્રા', 'પ્ત', 'કર્યો', 'છે', 'કાર', 'ણ', 'કે', 'તેઓ', 'એ', 'સ', 'તત', 'ઘણી', 'સં', 'પ', 'ત્તિ', 'નિ', 'ર્મા', 'ણની', 'વ', '્યૂ', 'હરચ', 'ના', 'ઓનો', 'ઉપયોગ', 'કર્યો', 'છે', 'કે', 'જે', 'માંથી', 'કોઈ', 'પણ', 'ઉપયોગ', 'કરી', 'શકે', 'છે', 'આજે', 'થી', 'શરૂ', 'અહીં', 'આગામી', 'બાર', 'ોમાં', 'થી', 'મિ', 'લિય', 'નેર', 'ના', 'બાર', 'લ', 'ક્ષણ', 'ો', 'છે', 'પટ', 'પટ', 'તી', 'પા', 'પણો', ':', 'લોકો', 'ક', 'ાદ', 'વ', 'માં', 'ન', 'હો', 'ઈ', 'શકે', 'આ', 'લે', 'ખ', 'માં', 'અ', 'મ', 'ે', 'તમ', 'ને', 'ક', 'હી', 'શ', 'ું', 'કે', 'સ્', 'વ', 'િ', 'મ', 'િ', 'ંગ', 'ને', 'વધ', 'ુ', 'ર', 'સ', 'પ્ર', 'દ', 'વધ', 'ુ', 'ખા', 'સ', 'બા', 'ંધ', 'ક', 'ામ', 'મિ', 'શ', '્રણ', 'વા', 'પ', 'રી', 'ને', 'મા', 'ળ', 'ભ', 'ર', 'વા', 'એક', 'મોટી', 'સ', 'ો', 'દ', 'ો', 'નથી', 'મા', 'ર્', 'કેટ', 'વ', 'ે', 'લ', '્યુ', 'ની', 'દ', '્રષ્', 'ટ', 'િ', 'એ', 'કો', 'ટક', 'મ', 'હિ', 'ન્દ્ર', 'ા', 'બે', 'ં', 'ક', 'એ', 'ચડી', 'એ', 'ફ', 'સી', 'બ', 'ાદ', 'બ', 'ી', 'જ', 'ા', 'ન', 'ં', 'બર', 'ની', '', '', '', 'SBI', 'એ', 'એ', 'ફ', 'ડી', 'ના', 'વ', '્યા', 'જ', 'દર', 'માં', '', '', 'ટ', 'કા', 'સુધ', 'ી', 'નો']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BPE IMPLEMENT vocab size 2K**"
      ],
      "metadata": {
        "id": "8XKGAtm4wiK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Train SentencePiece model\n",
        "spm.SentencePieceTrainer.train('--input=temp.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')\n",
        "\n",
        "# Load the trained model\n",
        "sp_bpe = spm.SentencePieceProcessor()\n",
        "sp_bpe.load('m_bpe.model')\n",
        "\n",
        "# Tokenize text from 'temp.txt' file\n",
        "print('*** BPE ***')\n",
        "with open('temp.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "bpe_list=sp_bpe.encode_as_pieces(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89zYqR6wwl0J",
        "outputId": "d32b0e23-195a-4fa4-c3ad-93a60aa592f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** BPE ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "tokens_cleaned = [token.replace('▁', '') for token in bpe_list if token not in punctuation_list ]\n",
        "\n",
        "# Count the frequency of each token\n",
        "token_freq = Counter(tokens_cleaned)\n",
        "\n",
        "bpe_token_freq_list = list(token_freq.items())\n",
        "# print(token_freq_list)\n",
        "bpe_sorted_token_freq_list = sorted(bpe_token_freq_list, key=lambda x: x[1], reverse=True)\n",
        "print(bpe_sorted_token_freq_list[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azPT5Vhkwm0i",
        "outputId": "2d0821ad-841a-4a67-bb25-ffe9f69f18be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('છે', 43), ('અને', 15), ('માટે', 10), ('સાથે', 9), ('જે', 8), ('પણ', 7), ('જ', 6), ('આ', 6), ('તે', 6), ('એક', 6), ('કે', 6), ('એ', 5), ('નથી', 4), ('પર', 4), ('કોઈ', 4), ('તેઓ', 4), ('કરી', 4), ('દ્વારા', 3), ('કરવામાં', 3), ('રીતે', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrqGb9vbFt1Q"
      },
      "source": [
        "**Unigram**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train('--input=temp.txt --model_prefix=m_unigram --vocab_size=500 --model_type=unigram')\n",
        "sp_unigram = spm.SentencePieceProcessor()\n",
        "sp_unigram.load('m_unigram.model')\n",
        "\n",
        "print('*** Unigram ***')\n",
        "with open('temp.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "with open('ques_25.txt', 'r', encoding='utf-8') as file:\n",
        "    text2 = file.read()\n",
        "\n",
        "ques_25_unigram=sp_bpe.encode_as_pieces(text2)\n",
        "unigram_list=sp_unigram.encode_as_pieces(text)\n",
        "# print(unigram_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFqxkdpeujfj",
        "outputId": "48ba1c15-809d-474a-f173-40c09671591f"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Unigram ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "tokens_cleaned = [token.replace('▁', '') for token in unigram_list if token not in punctuation_list and token not in matra ]\n",
        "ques_25_cleaned_unigram = [token.replace('▁', '') for token in ques_25_bpe if token not in punctuation_list ]\n",
        "\n",
        "# Count the frequency of each token\n",
        "token_freq = Counter(tokens_cleaned)\n",
        "\n",
        "unigram_token_freq_list = list(token_freq.items())\n",
        "# print(token_freq_list)\n",
        "unigram_sorted_token_freq_list = sorted(unigram_token_freq_list, key=lambda x: x[1], reverse=True)\n",
        "print(unigram_sorted_token_freq_list[:20])\n",
        "print(ques_25_cleaned_unigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjR8h3JLvFjo",
        "outputId": "2a7eefc5-43f2-4f2a-f75a-ce1229a6047e"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('', 64), ('છે', 44), ('જ', 36), ('ક', 33), ('ન', 30), ('ર', 30), ('માં', 29), ('ની', 29), ('સ', 27), ('આ', 26), ('ને', 25), ('પ', 22), ('એ', 22), ('ના', 21), ('વ', 21), ('બ', 21), ('ત', 21), ('મ', 18), ('હ', 17), ('અને', 16)]\n",
            "['ડ', 'િસ્', 'પ્', 'લે', 'પર', 'પણ', 'યા', 'ંત', '્ર', 'િક', 'ન', 'ુક', 'સાન', 'કો', 'ર્', 'નિંગ', 'ગ્લાસ', 'થી', 'એક', 'ખા', 'સ', 'ર', 'ક્ષ', 'ણા', 'ત્મક', 'સ્', 'તર', 'છે', '', '', 'ઇ', '1', '70', 'પા', 'ક', 'િસ્', 'તા', 'ન', 'ના', 'પીએ', 'મ', 'ઈ', 'મ', 'રા', 'ન', 'ખ', 'ાન', 'ની', 'અ', 'ધ', '્ય', 'ક્ષ', 'તા', 'માં', 'આ', 'બે', 'ઠ', 'ક', 'બ', 'ો', 'લા', 'વવા', 'માં', 'આવી', 'છે', 'આવી', 'ઘ', 'ટના', 'માં', 'છે', 'તરપિ', 'ંડી', 'કર', 'નાર', 'વ', '્ય', 'ક્તિ', 'મો', 'ટે', 'ભા', 'ગે', 'પરિ', 'ચિત', 'અથવા', 'તો', 'સં', 'બંધી', 'હોય', 'છે', 'એ', 'વા', 'ત', 'વધ', 'ારે', 'આ', 'ઘાત', 'જનક', 'હોય', 'છે', 'આ', 'પ', 'તાવટ', 'ની', 'આંતર', 'માળ', 'ખા', 'ખૂબ', 'સાર', 'ી', 'રીતે', 'વ', 'િક', 'સિત', 'નથી', 'આ', 'ર', 'ક', 'મ', 'આ', 'શ', 'ર', 'ે', 'છે', 'અ', 'મ', 'ે', 'આ', 'મા', 'મ', 'લે', 'બહ', 'ાર', 'નાં', 'વ', '્ય', 'ક્તિ', 'નાં', 'કોઇ', 'પણ', 'દ', 'ા', 'વા', 'ને', 'સં', 'પ', 'ૂ', 'ર્', 'ણ', 'રીતે', 'ફ', 'ગ', 'ા', 'વી', 'એ', 'છ', 'ી', 'એ', ')', '-', 'આ', 'ક', 'િસ્', 'સા', 'ઓ', 'માં', 'એન્', 'ટ', 'િ', 'બ', 'ાય', 'ો', 'ટ', 'િ', 'ક્', 'સ', 'સાથે', 'સારવાર', 'જ', 'રૂ', 'ર', 'છે', 'તમારી', 'ર', 'ા', 'શ', 'િ', 'ના', 'લોકો', 'એ', 'હ', 'ાલ', 'દ', 'ુ', 'ર્', 'ઘ', 'ટના', 'ઓ', 'થી', 'સા', 'વ', 'ધાન', 'રહે', 'વાની', 'જ', 'રૂ', 'ર', 'છે', 'આ', 'બાબત', 'છે', 'ત', '્ય', 'ાર', 'બ', 'ાદ', 'સ', 'ી', 'બી', 'આ', 'ઇ', 'અને', 'ઇ', 'ડી', 'એ', 'પૂ', 'રક', 'ચાર', '્', 'જ', 'શ', 'ી', 'ટ', 'દ', 'ાખ', 'લ', 'કરી', 'હ', 'તી', '', 'ઈ', 'ન્ડ', 'િયન', 'આ', 'ઈડલ', '11', 'ના', 'આગામી', 'એ', 'પિસ', 'ોડ', 'માં', 'ઉ', 'દિત', 'ન', 'ાર', 'ાયણ', 'અને', 'અ', 'લકા', 'યા', 'જ્ઞ', 'િક', 'મહે', 'માન', 'બનશે', 'જ્યારે', 'ક', '્લે', 'રિસ', 'અને', 'મિ', 'ત્સુ', 'ઇનો', 'સ', 'માન', 'હ', 'િસ્સ', 'ો', 'હતો', 'આથી', 'ઋ', 'ષિક', 'ેશ', 'ન', 'ગર', 'માં', 'વિ', 'રભ', 'દ્ર', 'બંધના', 'દ્', 'વાર', 'પાસે', 'તે', 'ફ', 'સાઈ', 'ગયો', 'હતો', 'ઉ', 'રી', 'હ', 'ુ', 'મલા', 'પછી', 'ભ', 'ાર', 'તી', 'ય', 'સ', 'ે', 'ના', 'ત', 'ર', 'ફ', 'થી', 'સ', 'ર્', 'જ', 'િક', 'લ', 'સ્', 'ટ્ર', 'ાઇ', 'ક', 'કરવામાં', 'આવી', 'આ', 'ખૂબ', 'જ', 'ખ', 'તર', 'ના', 'ક', 'મિ', 'શ', 'નને', 'સ', 'ફ', 'ળ', 'તા', 'પૂ', 'ર્વ', 'ક', 'પ', 'ાર', 'પા', 'ડ', 'વા', 'નો', 'શ્ર', 'ે', 'ય', 'જ', 'ાય', 'છે', 'ભ', 'ાર', 'ત', 'ના', 'ર', 'ા', 'ષ્ટ્ર', 'ી', 'ય', 'સુર', 'ક્ષ', 'ા', 'સ', 'લા', 'હ', 'કાર', 'અ', 'જી', 'ત', 'ડ', 'ો', 'વા', 'લ', 'વ', '્યા', 'પ', 'ક', 'દર', 'િયા', 'ક', 'િ', 'નાર', 'ો', 'જે', 'ના', 'માટે', 'સરકાર', 'ે', 'ઘણી', 'બ', 'ધ', 'ી', '', 'યો', 'જ', 'ના', 'ઓ', 'પણ', 'ઘ', 'ડી', 'છે', 'કે', 'ખ', 'ે', 'ડ', 'ૂ', 'તો', 'ને', 'થ', 'ોડા', 'ઘણા', 'અ', 'ં', 'શે', 'પ', 'ોતા', 'ની', 'ત', 'ક', 'લી', 'ફ', 'માં', 'ર', 'ા', 'હ', 'ત', 'મળી', 'રહે', 'સ્', 'પ', 'ે', 'ન્સ', 'રી', 'સ', 'શ', 'િ', 'ષ', '્ય', 'વૃ', 'ત્તિ', 'જો', 'તમે', 'સ', 'લ્', 'ફ', 'ર', 'માંથી', 'સા', 'મ', 'ય', 'િક', '', 'ટે', 'બલ', 'ને', '', 'ની', 'ચ', 'ે', 'ખસે', 'ડ', 'ો', 'તેમણે', 'કર', 'ોડો', 'પતિ', 'નો', 'દર', 'જ્જો', 'પ્રા', 'પ્ત', 'કર્યો', 'છે', 'કાર', 'ણ', 'કે', 'તેઓ', 'એ', 'સ', 'તત', 'ઘણી', 'સં', 'પ', 'ત્તિ', 'નિ', 'ર્મા', 'ણની', 'વ', '્યૂ', 'હરચ', 'ના', 'ઓનો', 'ઉપયોગ', 'કર્યો', 'છે', 'કે', 'જે', 'માંથી', 'કોઈ', 'પણ', 'ઉપયોગ', 'કરી', 'શકે', 'છે', 'આજે', 'થી', 'શરૂ', 'અહીં', 'આગામી', 'બાર', 'ોમાં', 'થી', 'મિ', 'લિય', 'નેર', 'ના', 'બાર', 'લ', 'ક્ષણ', 'ો', 'છે', 'પટ', 'પટ', 'તી', 'પા', 'પણો', ':', 'લોકો', 'ક', 'ાદ', 'વ', 'માં', 'ન', 'હો', 'ઈ', 'શકે', 'આ', 'લે', 'ખ', 'માં', 'અ', 'મ', 'ે', 'તમ', 'ને', 'ક', 'હી', 'શ', 'ું', 'કે', 'સ્', 'વ', 'િ', 'મ', 'િ', 'ંગ', 'ને', 'વધ', 'ુ', 'ર', 'સ', 'પ્ર', 'દ', 'વધ', 'ુ', 'ખા', 'સ', 'બા', 'ંધ', 'ક', 'ામ', 'મિ', 'શ', '્રણ', 'વા', 'પ', 'રી', 'ને', 'મા', 'ળ', 'ભ', 'ર', 'વા', 'એક', 'મોટી', 'સ', 'ો', 'દ', 'ો', 'નથી', 'મા', 'ર્', 'કેટ', 'વ', 'ે', 'લ', '્યુ', 'ની', 'દ', '્રષ્', 'ટ', 'િ', 'એ', 'કો', 'ટક', 'મ', 'હિ', 'ન્દ્ર', 'ા', 'બે', 'ં', 'ક', 'એ', 'ચડી', 'એ', 'ફ', 'સી', 'બ', 'ાદ', 'બ', 'ી', 'જ', 'ા', 'ન', 'ં', 'બર', 'ની', '', '', '', 'SBI', 'એ', 'એ', 'ફ', 'ડી', 'ના', 'વ', '્યા', 'જ', 'દર', 'માં', '', '', 'ટ', 'કા', 'સુધ', 'ી', 'નો']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IndicBERT**"
      ],
      "metadata": {
        "id": "xWnmSzLkwU7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xo9uwuwwuTB",
        "outputId": "00d79ae4-3df8-441f-86a4-c8b5eaa9087d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert',keep_accents=True)\n",
        "model = AutoModel.from_pretrained('ai4bharat/indic-bert')\n"
      ],
      "metadata": {
        "id": "wwNMKNEayPHf"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('*** IndicBert ***')\n",
        "# sentence = \"લોકો બેંકોની બહાર ઊભાં રહી ગયાં હતાં\"\n",
        "# tokenized_input = tokenizer(sentence)\n",
        "\n",
        "\n",
        "\n",
        "with open('temp.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "# Tokenize a sentence\n",
        "with open('ques_25.txt', 'r', encoding='utf-8') as file:\n",
        "    text2 = file.read()\n",
        "tokens = tokenizer(text)\n",
        "tokens2 = tokenizer(text2)\n",
        "indicBert_list=tokenizer.convert_ids_to_tokens(tokens['input_ids'])\n",
        "indicBert_list2=tokenizer.convert_ids_to_tokens(tokens2['input_ids'])\n",
        "# print(\"segmented input sentence \",tokenizer.convert_ids_to_tokens(tokens['input_ids']))\n",
        "\n",
        "indicBert_tokens_cleaned = [token.replace('▁', '') for token in indicBert_list if token not in punctuation_list and token not in matra ]\n",
        "indicBert_tokens2_cleaned = [token.replace('▁', '') for token in indicBert_list2 if token not in punctuation_list and token not in matra ]\n",
        "\n",
        "# Count the frequency of each token\n",
        "indicBert_token_freq = Counter(indicBert_tokens_cleaned)\n",
        "\n",
        "indicBert_token_freq_list = list(indicBert_token_freq.items())\n",
        "# print(token_freq_list)\n",
        "indicBert_sorted_token_freq_list = sorted(indicBert_token_freq_list, key=lambda x: x[1], reverse=True)\n",
        "print(indicBert_sorted_token_freq_list[:20])\n",
        "print(indicBert_tokens2_cleaned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azkGRiIDzPVi",
        "outputId": "3b6d6738-6f36-408a-df40-1aea01628d0c"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** IndicBert ***\n",
            "[('છે', 43), ('ની', 17), ('અને', 15), ('ને', 12), ('માટે', 11), ('નો', 10), ('', 9), ('માં', 9), ('સાથે', 9), ('ના', 8), ('એ', 8), ('જે', 8), ('જ', 7), ('આ', 7), ('કે', 7), ('પણ', 7), ('થી', 6), ('તા', 6), ('તે', 6), ('એક', 6)]\n",
            "['[CLS]', 'ડિસ્પ્લે', 'પર', 'પણ', 'યાંત્રિક', 'નુકસાન', 'કોર્ન', 'િંગ', 'ગ્લાસ', 'થી', 'એક', 'ખાસ', 'રક્ષણ', 'ાત્મક', 'સ્તર', 'છે', '4', '(', '2017', '):', 'ઇ', '12', '70', 'પાકિસ્તાનના', 'પીએમ', 'ઈમરાન', 'ખાન', 'ની', 'અધ્યક્ષતા', 'માં', 'આ', 'બેઠક', 'બોલાવવા', 'માં', 'આવી', 'છે', 'આવી', 'ઘટનામાં', 'છેતરપિંડી', 'કરનાર', 'વ્યક્તિ', 'મોટે', 'ભાગે', 'પરિચિત', 'અથવા', 'તો', 'સંબંધી', 'હોય', 'છે', 'એ', 'વાત', 'વધારે', 'આઘાત', 'જનક', 'હોય', 'છે', 'આ', 'પતાવટ', 'ની', 'આંતર', 'માળ', 'ખા', 'ખૂબ', 'સારી', 'રીતે', '', 'વિકસિત', 'નથી', 'આ', 'રકમ', 'આશરે', 'છે', 'અમે', 'આ', 'મામલે', 'બહાર', 'નાં', 'વ્યક્તિ', 'નાં', 'કોઇ', 'પણ', 'દાવા', 'ને', 'સંપૂર્ણ', 'રીતે', 'ફગાવી', 'એ', 'છીએ', ')', '-', 'આ', 'કિસ્સાઓમાં', 'એન્ટિબાયોટિક્સ', 'સાથે', 'સારવાર', 'જરૂર', 'છે', 'તમારી', 'રાશિના', 'લોકોએ', 'હાલ', 'દુર્ઘટના', 'ઓ', 'થી', 'સાવધાન', 'રહેવાની', 'જરૂર', 'છે', 'આ', 'બાબત', 'છે', 'ત્યારબાદ', 'સીબીઆઇ', 'અને', 'ઇ', 'ડીએ', 'પૂરક', 'ચાર્જ', 'શીટ', 'દાખલ', 'કરી', 'હતી', '.', 'ઈન્ડિયન', 'આઈ', 'ડલ', '11', 'ના', 'આગામી', 'એપિસોડ', 'માં', 'ઉદ', 'િત', 'નારાયણ', 'અને', 'અલ', 'કા', 'યા', 'જ્ઞ', 'િક', 'મહેમાન', 'બનશે', 'જ્યારે', 'ક્લે', 'રિસ', 'અને', 'મિ', 'ત્સ', 'ઇ', 'નો', 'સમાન', 'હિસ્સો', 'હતો', 'આથી', 'ઋષિ', 'કેશ', 'નગરમાં', 'વિર', 'ભદ્ર', 'બંધ', 'ના', 'દ્વાર', 'પાસે', 'તે', 'ફસા', 'ઈ', 'ગયો', 'હતો', 'ઉ', 'રી', 'હુમલા', 'પછી', 'ભારતીય', 'સેના', 'તરફથી', 'સર્જ', 'િકલ', 'સ્ટ્રાઇક', 'કરવામાં', 'આવી', 'આ', 'ખૂબ', 'જ', 'ખતરનાક', 'મિશન', 'ને', 'સફળતા', 'પૂર્વક', 'પાર', 'પાડવા', 'નો', 'શ્રેય', 'જાય', 'છે', 'ભારતના', 'રાષ્ટ્રીય', 'સુરક્ષા', 'સલાહકાર', 'અ', 'જીત', 'ડો', 'વાલ', 'વ્યાપક', 'દરિયાકિનાર', 'જેના', 'માટે', 'સરકારે', 'ઘણી', 'બધી', 'યોજનાઓ', 'પણ', 'ઘડી', 'છે', 'કે', 'ખેડૂતો', 'ને', 'થોડા', 'ઘણા', 'અંશ', 'પોતાની', 'તકલીફ', 'માં', 'રાહત', 'મળી', 'રહે', 'સ્પેન', '્સ', 'રીસ', 'શિષ્ય', 'વૃત્તિ', 'જો', 'તમે', 'સલ', '્ફર', 'માંથી', 'સામયિક', 'ટેબલ', 'ને', 'નીચે', 'ખસેડ', 'તેમણે', 'કરોડો', 'પતિ', 'નો', 'દરજ્જો', 'પ્રાપ્ત', 'કર્યો', 'છે', 'કારણ', 'કે', 'તેઓએ', 'સતત', 'ઘણી', 'સંપત્તિ', 'નિર્માણ', 'ની', 'વ્યૂહરચના', 'ઓનો', 'ઉપયોગ', 'કર્યો', 'છે', 'કે', 'જેમાં', 'થી', 'કોઈ', 'પણ', 'ઉપયોગ', 'કરી', 'શકે', 'છે', 'આજે', 'થી', 'શરૂ', 'અહીં', 'આગામી', 'બાર', 'ોમાંથી', 'મિલિયન', 'ેર', 'ના', 'બાર', 'લક્ષણો', 'છે', 'પટ', 'પટ', 'તી', 'પાપ', 'ણો', ':', 'લોકો', 'ક', 'ાદવ', 'માં', 'ન', 'હોઈ', 'શકે', 'આ', 'લેખમાં', 'અમે', 'તમ', 'ને', 'કહી', 'શું', 'કે', 'સ્વિમિંગ', 'ને', 'વધુ', 'રસપ્રદ', 'વધુ', 'ખાસ', 'બાંધકામ', 'મિશ્રણ', 'વાપરી', 'ને', 'માળ', 'ભરવા', 'એક', 'મોટી', 'સોદો', 'નથી', 'માર્કેટ', 'વેલ્યુ', 'ની', 'દ્રષ્ટિએ', 'કોટ', 'ક', 'મહિન્દ્રા', 'બેંક', 'એચડી', 'એફસી', 'બાદ', 'બીજા', 'નંબર', 'ની', '.', '.', 's', 'bi', 'એ', 'એફ', 'ડીના', 'વ્યાજ', 'દર', 'માં', '0.25', 'ટકા', 'સુધીનો', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**mBERT**"
      ],
      "metadata": {
        "id": "4t5V5S--1yCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYi62_ia1xSo",
        "outputId": "fbddf1e8-af16-4087-feea-4f4a2873e611"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentence = \"તમારું નામ શું છે?\"\n",
        "tokens = tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "id": "wZudDnF7D_Pp"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsmDFDVC1tK3",
        "outputId": "b4e46212-9798-4941-cc28-70cee1af4797"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ત', '##મ', '##ાર', '##ું', 'નામ', 'શ', '##ું', 'છે', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9pnLtRJTFyD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WhitespaceTokenizer**"
      ],
      "metadata": {
        "id": "MY89ilRMFnEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import WhitespaceTokenizer() method from nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "# Create a reference variable for Class WhitespaceTokenizer\n",
        "tk = WhitespaceTokenizer()\n",
        "\n",
        "# # Create a string input\n",
        "# gfg = \" લોકો બેંકોની બહાર ઊભાં રહી ગયાં હતાં\"\n",
        "\n",
        "# # Use tokenize method\n",
        "# geek = tk.tokenize(gfg)\n",
        "\n",
        "# print(geek)\n",
        "print('*** WhitespaceTokenizer ***')\n",
        "# sentence = \"લોકો બેંકોની બહાર ઊભાં રહી ગયાં હતાં\"\n",
        "# tokenized_input = tokenizer(sentence)\n",
        "\n",
        "\n",
        "\n",
        "with open('temp.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "# Tokenize a sentence\n",
        "with open('ques_25.txt', 'r', encoding='utf-8') as file:\n",
        "    text2 = file.read()\n",
        "whitespace_list = tk.tokenize(text)\n",
        "whitespace_list_ques_25 = tk.tokenize(text2)\n",
        "\n",
        "# print(\"segmented input sentence \",tokenizer.convert_ids_to_tokens(tokens['input_ids']))\n",
        "\n",
        "whitespace_tokens_cleaned = [token.replace('▁', '') for token in whitespace_list if token not in punctuation_list and token not in matra ]\n",
        "whitespace_tokens_cleaned_ques_25 = [token.replace('▁', '') for token in whitespace_list_ques_25 if token not in punctuation_list and token not in matra ]\n",
        "\n",
        "# Count the frequency of each token\n",
        "whitespace_token_freq = Counter(whitespace_tokens_cleaned)\n",
        "\n",
        "whitespace_token_freq_list = list(whitespace_token_freq.items())\n",
        "# print(token_freq_list)\n",
        "whitespace_sorted_token_freq_list = sorted(whitespace_token_freq_list, key=lambda x: x[1], reverse=True)\n",
        "print(whitespace_sorted_token_freq_list[:20])\n",
        "print(whitespace_tokens_cleaned_ques_25)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaOGizkEFycc",
        "outputId": "23d788da-a284-431c-daec-4e7e1d67deb1"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** WhitespaceTokenizer ***\n",
            "[('છે.', 23), ('અને', 15), ('છે', 12), ('માટે', 10), ('જે', 8), ('સાથે', 7), ('પણ', 7), ('જ', 6), ('આ', 6), ('એક', 6), ('કે', 6), ('તે', 5), ('એ', 5), ('છે,', 5), ('કોઈ', 4), ('તેઓ', 4), ('કરી', 4), ('દ્વારા', 3), ('કરવામાં', 3), ('રીતે', 3)]\n",
            "['ડિસ્પ્લે', 'પર', 'પણ', 'યાંત્રિક', 'નુકસાન', 'કોર્નિંગ', 'ગ્લાસથી', 'એક', 'ખાસ', 'રક્ષણાત્મક', 'સ્તર', 'છે.', '(2017):', 'ઇ', '1270.', 'પાકિસ્તાનના', 'પીએમ', 'ઈમરાન', 'ખાનની', 'અધ્યક્ષતામાં', 'આ', 'બેઠક', 'બોલાવવામાં', 'આવી', 'છે.', 'આવી', 'ઘટનામાં', 'છેતરપિંડી', 'કરનાર', 'વ્યક્તિ', 'મોટે', 'ભાગે', 'પરિચિત', 'અથવા', 'તો', 'સંબંધી', 'હોય', 'છે', 'એ', 'વાત', 'વધારે', 'આઘાતજનક', 'હોય', 'છે.', 'આ', 'પતાવટની', 'આંતરમાળખા', 'ખૂબ', 'સારી', 'રીતે', 'વિકસિત', 'નથી.', 'આ', 'રકમ', 'આશરે', 'છે.', 'અમે', 'આ', 'મામલે', 'બહારનાં', 'વ્યક્તિનાં', 'કોઇ', 'પણ', 'દાવાને', 'સંપૂર્ણ', 'રીતે', 'ફગાવીએ', 'છીએ.', 'આ', 'કિસ્સાઓમાં', 'એન્ટિબાયોટિક્સ', 'સાથે', 'સારવાર', 'જરૂર', 'છે.', 'તમારી', 'રાશિના', 'લોકોએ', 'હાલ', 'દુર્ઘટનાઓથી', 'સાવધાન', 'રહેવાની', 'જરૂર', 'છે.', 'આ', 'બાબત', 'છે.', 'ત્યારબાદ', 'સીબીઆઇ', 'અને', 'ઇડીએ', 'પૂરક', 'ચાર્જશીટ', 'દાખલ', 'કરી', 'હતી', 'ઈન્ડિયન', 'આઈડલ', '11ના', 'આગામી', 'એપિસોડમાં', 'ઉદિત', 'નારાયણ', 'અને', 'અલકા', 'યાજ્ઞિક', 'મહેમાન', 'બનશે.', 'જ્યારે', 'ક્લેરિસ', 'અને', 'મિત્સુઇનો', 'સમાન', 'હિસ્સો', 'હતો.', 'આથી', 'ઋષિકેશ', 'નગરમાં', 'વિરભદ્ર', 'બંધના', 'દ્વાર', 'પાસે', 'તે', 'ફસાઈ', 'ગયો', 'હતો.', 'ઉરી', 'હુમલા', 'પછી', 'ભારતીય', 'સેના', 'તરફથી', 'સર્જિકલ', 'સ્ટ્રાઇક', 'કરવામાં', 'આવી.', 'આ', 'ખૂબ', 'જ', 'ખતરનાક', 'મિશનને', 'સફળતા', 'પૂર્વક', 'પાર', 'પાડવાનો', 'શ્રેય', 'જાય', 'છે', 'ભારતના', 'રાષ્ટ્રીય', 'સુરક્ષા', 'સલાહકાર', 'અજીત', 'ડોવાલ.', 'વ્યાપક', 'દરિયાકિનારો.', 'જેના', 'માટે', 'સરકારે', 'ઘણી', 'બધી', 'યોજનાઓ', 'પણ', 'ઘડી', 'છે', 'કે', 'ખેડૂતોને', 'થોડા', 'ઘણા', 'અંશે', 'પોતાની', 'તકલીફમાં', 'રાહત', 'મળી', 'રહે.', 'સ્પેન્સ', 'રીસ', 'શિષ્યવૃત્તિ.', 'જો', 'તમે', 'સલ્ફરમાંથી', 'સામયિક', 'ટેબલને', 'નીચે', 'ખસેડો.', 'તેમણે', 'કરોડોપતિનો', 'દરજ્જો', 'પ્રાપ્ત', 'કર્યો', 'છે', 'કારણ', 'કે', 'તેઓએ', 'સતત', 'ઘણી', 'સંપત્તિ', 'નિર્માણની', 'વ્યૂહરચનાઓનો', 'ઉપયોગ', 'કર્યો', 'છે', 'કે', 'જેમાંથી', 'કોઈ', 'પણ', 'ઉપયોગ', 'કરી', 'શકે', 'છે-', 'આજેથી', 'શરૂ', 'અહીં', 'આગામી', 'બારોમાંથી', 'મિલિયનેરના', 'બાર', 'લક્ષણો', 'છે:', 'પટ', 'પટતી', 'પાપણો', 'લોકો', 'કાદવ', 'માં', 'ન', 'હોઈ', 'શકે.', 'આ', 'લેખમાં', 'અમે', 'તમને', 'કહીશું', 'કે', 'સ્વિમિંગને', 'વધુ', 'રસપ્રદ.', 'વધુ', 'ખાસ', 'બાંધકામ', 'મિશ્રણ', 'વાપરીને', 'માળ', 'ભરવા', 'એક', 'મોટી', 'સોદો', 'નથી.', 'માર્કેટ', 'વેલ્યુની', 'દ્રષ્ટિએ', 'કોટક', 'મહિન્દ્રા', 'બેંક', 'એચડીએફસી', 'બાદ', 'બીજા', 'નંબરની.', 'SBIએ', 'એફડીના', 'વ્યાજ', 'દરમાં', '0.25', 'ટકા', 'સુધીનો.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5  find the precision, recall and F-score for the 25 sentences.**"
      ],
      "metadata": {
        "id": "9i-chccxNHXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_labels = ['ગ્લાસથી', 'એક', 'રક્ષણાત્મક સ્તર', 'છે', 'યાંત્રિક નુકસાન', 'કોર્નિંગ', 'આ', 'પાકિસ્તાનના', 'ઈમરાન ખાનની', 'બોલાવવામાં', 'આવી છે', 'પીએમ', 'છેતરપિંડી', 'હોય છે', 'આઘાતજનક', 'આ', 'સારી', 'વિકસિત', 'પતાવટની', 'ખૂબ', 'આંતરમાળખા', 'આ', 'છે', 'આશરે', 'ફગાવીએ', 'સંપૂર્ણ', 'દાવાને', 'વ્યક્તિનાં', 'અમે', 'આ', 'મામલે', 'છીએ', 'આ', 'કિસ્સાઓમાં', 'જરૂર', 'છે', 'એન્ટિબાયોટિક્સ', 'તમારી', 'રાશિના', 'લોકોએ', 'દુર્ઘટનાઓથી', 'રહેવાની', 'જરૂર', 'હાલ', 'છે', 'આ', 'છે', 'બાબત', 'ઇડીએ', 'પૂરક', 'હતી', 'ચાર્જશીટ', 'ઈન્ડિયન આઈડલ 11ના', 'એપિસોડમાં', 'મહેમાન', 'બનશે', 'મિત્સુઇનો', 'હતો', 'બંધના', 'નગરમાં', 'હતો', 'મિશનને', 'ખતરનાક', 'પાડવાનો', 'ભારતના', 'સફળતા', 'આવી', 'વ્યાપક', 'દરિયાકિનારો', 'સરકારે', 'તકલીફમાં', 'ખેડૂતોને', 'થોડા ઘણા', 'અંશે પોતાની તકલીફમાં', 'યોજનાઓ', 'રહે', 'શિષ્યવૃત્તિ', 'સલ્ફરમાંથી', 'સામયિક ટેબલને', 'પ્રાપ્ત કર્યો', 'નિર્માણની', 'વ્યૂહરચનાઓનો', 'આજેથી', 'તેમણે', 'બારોમાંથી મિલિયનેરના', 'છે', 'મિલિયનેર', 'પાપણો', 'લોકો', 'શકે', 'લેખમાં', 'અમે', 'તમને', 'કહીશું', 'સ્વિમિંગને', 'વધુ', 'રસપ્રદ', 'વાપરીને', 'એક', 'ખાસ', 'મોટી મિશ્રણ વેલ્યુની', 'બીજા નંબરની', 'એફડીના', 'સુધીનો', 'એચડીએફસી', 'SBI', 'એફડી']\n",
        "\n",
        "print(ground_truth_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gWWlgUGNyzo",
        "outputId": "9aba1f91-6ed9-45a8-b267-b418a1807aec"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ગ્લાસથી', 'એક', 'રક્ષણાત્મક સ્તર', 'છે', 'યાંત્રિક નુકસાન', 'કોર્નિંગ', 'આ', 'પાકિસ્તાનના', 'ઈમરાન ખાનની', 'બોલાવવામાં', 'આવી છે', 'પીએમ', 'છેતરપિંડી', 'હોય છે', 'આઘાતજનક', 'આ', 'સારી', 'વિકસિત', 'પતાવટની', 'ખૂબ', 'આંતરમાળખા', 'આ', 'છે', 'આશરે', 'ફગાવીએ', 'સંપૂર્ણ', 'દાવાને', 'વ્યક્તિનાં', 'અમે', 'આ', 'મામલે', 'છીએ', 'આ', 'કિસ્સાઓમાં', 'જરૂર', 'છે', 'એન્ટિબાયોટિક્સ', 'તમારી', 'રાશિના', 'લોકોએ', 'દુર્ઘટનાઓથી', 'રહેવાની', 'જરૂર', 'હાલ', 'છે', 'આ', 'છે', 'બાબત', 'ઇડીએ', 'પૂરક', 'હતી', 'ચાર્જશીટ', 'ઈન્ડિયન આઈડલ 11ના', 'એપિસોડમાં', 'મહેમાન', 'બનશે', 'મિત્સુઇનો', 'હતો', 'બંધના', 'નગરમાં', 'હતો', 'મિશનને', 'ખતરનાક', 'પાડવાનો', 'ભારતના', 'સફળતા', 'આવી', 'વ્યાપક', 'દરિયાકિનારો', 'સરકારે', 'તકલીફમાં', 'ખેડૂતોને', 'થોડા ઘણા', 'અંશે પોતાની તકલીફમાં', 'યોજનાઓ', 'રહે', 'શિષ્યવૃત્તિ', 'સલ્ફરમાંથી', 'સામયિક ટેબલને', 'પ્રાપ્ત કર્યો', 'નિર્માણની', 'વ્યૂહરચનાઓનો', 'આજેથી', 'તેમણે', 'બારોમાંથી મિલિયનેરના', 'છે', 'મિલિયનેર', 'પાપણો', 'લોકો', 'શકે', 'લેખમાં', 'અમે', 'તમને', 'કહીશું', 'સ્વિમિંગને', 'વધુ', 'રસપ્રદ', 'વાપરીને', 'એક', 'ખાસ', 'મોટી મિશ્રણ વેલ્યુની', 'બીજા નંબરની', 'એફડીના', 'સુધીનો', 'એચડીએફસી', 'SBI', 'એફડી']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11yaTik4LLcHP867YIU90eO2CYqkSSe3O",
      "authorship_tag": "ABX9TyOytoU1bSTlII4FWUY1puLV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}