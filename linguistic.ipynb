{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/padariya-tech/Opencv_codes/blob/main/linguistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHnbyVzqiftH"
      },
      "source": [
        "# **Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTpJ73NvVhuM",
        "outputId": "60bbf817-78a1-4d1b-b66d-0800b56d0ad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-c9242a88dfc1>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  data = pd.read_csv('/content/drive/MyDrive/gu_100.txt',on_bad_lines='skip', sep='delimiter',header = None, encoding = 'utf-8')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   0\n",
            "0  ૯મી ઓગસ્ટ ૨૦૧૬ના રોજ આદિવાસી વિકાસ સંગઠન દ્વાર...\n",
            "1  આ પતાવટની આંતરમાળખા ખૂબ સારી રીતે વિકસિત નથી, ...\n",
            "2  વહીવટ બિલ્ડિંગ નજીક પાછળના બાજુ પર, હોટેલ આંતર...\n",
            "3  ગુરુવારે સવારે બેંકો ખુલતા પહેલા પ્રતિબંધિત નો...\n",
            "4  ઈન્ડિયન આઈડલ 11ના આગામી એપિસોડમાં ઉદિત નારાયણ ...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_csv('/content/drive/MyDrive/gu_100.txt',on_bad_lines='skip', sep='delimiter',header = None, encoding = 'utf-8')\n",
        "#importing dataframe\n",
        "\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI3WyiJQikX4"
      },
      "source": [
        "**https://learngujaratiwithme.com/gujarati-vowels/ getting list from this website**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q3frmWd6GHgN"
      },
      "outputs": [],
      "source": [
        "swar = ['અ', 'આ', 'ઇ', 'ઈ', 'ઉ', 'ઊ', 'ઋ', 'ઌ', 'ઍ', 'એ', 'ઐ', 'ઑ', 'ઓ', 'ઔ','અં','અઃ']\n",
        "matra = ['ા', 'િ', 'ી', 'ુ', 'ૂ', 'ૃ', 'ૄ', 'ૅ', 'ે', 'ૈ', 'ૉ', 'ો', 'ૌ']\n",
        "vyanjan = ['ક', 'ખ', 'ગ', 'ઘ', 'ઙ', 'ચ', 'છ', 'જ', 'ઝ', 'ઞ', 'ટ', 'ઠ', 'ડ', 'ઢ', 'ણ', 'ત', 'થ', 'દ', 'ધ', 'ન', 'પ', 'ફ', 'બ', 'ભ', 'મ', 'ય', 'ર', 'લ', 'ળ', 'વ', 'શ', 'ષ', 'સ', 'હ']\n",
        "gujarati_dict = {\n",
        "    'ા': 'આ',\n",
        "    'િ': 'ઇ',\n",
        "    'ી': 'ઈ',\n",
        "    'ુ': 'ઉ',\n",
        "    'ૂ': 'ઊ',\n",
        "    'ૃ': 'ઋ',\n",
        "    'ૄ': 'ઌ',\n",
        "    'ૅ': 'ઍ',\n",
        "    'ે': 'એ',\n",
        "    'ૈ': 'ઐ',\n",
        "    'ૉ': 'ઑ',\n",
        "    'ો': 'ઓ',\n",
        "    'ૌ': 'ઔ',\n",
        "    'ં':'અં',\n",
        "    'ઃ':'અઃ'\n",
        "}\n",
        "\n",
        "word = \"માત્ર\"\n",
        "punctuation_list = [\n",
        "'>्','0्', '1्', '\"्', 'ळ्', '2्', '–्', 'ः्', '3्', '5्', '4्', '््', '%्', '—्', '8्', '6्', '7्', '9्', 'ॅ्', 'a्', '>्', 'e्', '#्', 'i्','r्', 't्', '»्', 'o्', 'n्', 'd्', '०्', 's्', 'h्', 'l्', 'c्', 'm्',\n",
        "'\\u200c्', '\\u200b्', 'ï्', 'A्', 'p्', '•्', 'b्', 'G्', 'B्', '&्', 'u्',\n",
        "'_्', '@्', 'M्', 'о्', 'f्', '·्', '$्', 'S्', 'g्', 'I्', 'а्', 'е्', 'P्',\n",
        "'и्', 'R्', 'y्', 'k्', 'w्', 'T्', '�्', 'a','b','c','d','e','f','g','h','i','j',\n",
        " 'k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','0','1','2','3','4','5','6','7','8','9',\n",
        " 'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z',\n",
        " '!','\"','\\\\','#','\\\\','$','%','\\\\','&',\"'\",'\\\\','(','\\\\',')','\\\\','*','\\\\','+',',','\\\\','-','\\\\','.','/',\n",
        " ':',';','<','=','>','\\\\','?','@','\\\\','[','\\\\','\\\\','\\\\',']','\\\\','^','_','`','\\\\','{','\\\\','|','\\\\','}','\\\\','~']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmb1OXL9ivTq"
      },
      "source": [
        "# **Q1 Perform the Unicode correction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omgqajwTjEgw"
      },
      "source": [
        "**wrote function for unicode correction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KpZUtdm5-R8H"
      },
      "outputs": [],
      "source": [
        "\n",
        "def correct_unicode(word):\n",
        "  correct_word=\"\"\n",
        "  length = len(word)\n",
        "  for i in range(length):\n",
        "\n",
        "        if word[i] in vyanjan:\n",
        "          correct_word +=' '+word[i]+\"્\"\n",
        "          if i<=length-2 and (word[i+1]=='्' or word[i+1] in punctuation_list):\n",
        "            pass\n",
        "          elif i<=length-2 and word[i+1] in matra:\n",
        "            correct_word +=' ' +gujarati_dict[word[i+1]]\n",
        "          elif i<=length-2 and (word[i+1] in vyanjan or word[i+1] in swar) :\n",
        "            correct_word +=' ' +'અ'\n",
        "          elif i == length - 1:\n",
        "                correct_word +=' ' +'અ'\n",
        "        elif word[i] in swar :\n",
        "          correct_word +=' ' +word[i]\n",
        "\n",
        "\n",
        "  return correct_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlsBPfyjju5K"
      },
      "source": [
        "**print for first 10 row from data file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2s8bd2-SR9Pw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "abc3975b-6ba6-40b5-8c1e-80685a1658c9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vyanjan' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b7b8d1ce0962>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Process each word and store cleaned versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mcleaned_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Explain what this function does\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mprocessed_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# print(f\"{word} => {cleaned_word}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e1184fe23b9c>\u001b[0m in \u001b[0;36mcorrect_unicode\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvyanjan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m           \u001b[0mcorrect_word\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"્\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'्'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuation_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vyanjan' is not defined"
          ]
        }
      ],
      "source": [
        "counter = 0\n",
        "processed_words = [] # list contain all the corrected unicode\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    # Check if we have processed 10 rows\n",
        "    # if counter >= 10 :\n",
        "        # break\n",
        "\n",
        "    # Get the text data from the current row\n",
        "    text = row[0]\n",
        "\n",
        "    # Split the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Process each word and store cleaned versions\n",
        "    for word in words:\n",
        "        cleaned_word = correct_unicode(word)  # Explain what this function does\n",
        "        processed_words.append(cleaned_word)\n",
        "        # print(f\"{word} => {cleaned_word}\")\n",
        "\n",
        "    # Increment the counter\n",
        "    # counter += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BJeaf6peCYO"
      },
      "outputs": [],
      "source": [
        "N = 100\n",
        "result=[]\n",
        "result = processed_words[:N]\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2GKqR6hWAmo"
      },
      "source": [
        "# **Q2 Find all characters and syllables. Store a list of them in descending order of their frequencies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM6wpghIhq4T"
      },
      "source": [
        "# **uni-gram and bi-gram frequencies of Character**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fR6WzHfoTlQe"
      },
      "outputs": [],
      "source": [
        "# craeted global list to store frequency for character\n",
        "unigram_char_list=[]\n",
        "\n",
        "unigram_char_list_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def unigram_for_char(each_word, vyanjan_list,unigram_char_list):\n",
        "  # Join the strings and remove spaces\n",
        "  joined_string = ''.join(each_word).replace(\" \", \"\")\n",
        "  # print(joined_string)\n",
        "  # Count character frequencies, treating '્' as part of the preceding character\n",
        "  char_counts = Counter(\n",
        "      c for c in joined_string if c != \"્\" or c == joined_string[joined_string.index(c) - 1]\n",
        "  )\n",
        "\n",
        "  # Sort characters by frequency (descending)\n",
        "  sorted_chars = sorted(char_counts, key=char_counts.get, reverse=True)\n",
        "\n",
        "  # Create a list of characters with halant (if needed)\n",
        "  # char_freq_list = [\n",
        "  #     (char + \"્\" if char in vyanjan_list else char, char_counts[char])\n",
        "  #     for char in sorted_chars\n",
        "  # ]\n",
        "\n",
        "  for char, freq in char_counts.items():\n",
        "        char_key = char + \"્\" if char in vyanjan_list else char\n",
        "        found = False\n",
        "        for i, (existing_char, existing_freq) in enumerate(unigram_char_list):\n",
        "            if existing_char == char_key:\n",
        "                unigram_char_list[i] = (existing_char, existing_freq + freq)\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            unigram_char_list.append((char_key, freq))\n",
        "\n",
        "def sort_list_by_frequency(char_freq_list):\n",
        "    return sorted(char_freq_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for word in processed_words:\n",
        "\n",
        "  unigram_for_char(word, vyanjan,unigram_char_list) # unigram for character\n",
        "\n",
        "  # print(\" unigram characters \")\n",
        "\n",
        "unigram_char_list_sorted=sort_list_by_frequency(unigram_char_list)\n",
        "\n",
        "print(unigram_char_list_sorted[:20])"
      ],
      "metadata": {
        "id": "FaFV1bNSeHmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbxJT60oittx",
        "outputId": "6b44763b-b45d-4b92-c5b5-9b4e412fa321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('અ', 6781001), ('આ', 3692279), ('એ', 2522856), ('ર્', 2128526), ('ન્', 1734112), ('ઈ', 1589073), ('ક્', 1386555), ('મ્', 1324843), ('ઓ', 1236360), ('ત્', 1207724), ('વ્', 1149790), ('સ્', 976491), ('પ્', 867048), ('ઇ', 838885), ('ય્', 745130), ('લ્', 684911), ('ઉ', 673954), ('જ્', 585042), ('ટ્', 520557), ('હ્', 516880)]\n"
          ]
        }
      ],
      "source": [
        "print(unigram_char_list_sorted[:20])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WSzwspIav4wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_char_list=[]\n",
        "bigram_char_list_sorted=[]\n",
        "def sort_list_by_frequency(char_freq_list):\n",
        "    return sorted(char_freq_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "def bigram_for_char(each_word, vyanjan_list,bigram_char_list):\n",
        "\n",
        "    bigram_counts = {}\n",
        "    joined_string = ''.join(each_word).replace(\" \", \"\")\n",
        "\n",
        "    for i in range(len(joined_string) - 1):\n",
        "        char1 = joined_string[i]\n",
        "        char2 = joined_string[i + 1]\n",
        "\n",
        "        if char1 == \"્\":\n",
        "            continue\n",
        "\n",
        "        elif char2 == \"્\":\n",
        "            if i + 2 < len(joined_string) and joined_string[i + 2] != \"્\":\n",
        "                bigram = char1 + joined_string[i + 1] + joined_string[i + 2]\n",
        "                i += 1\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        else:\n",
        "            bigram = char1 + char2\n",
        "\n",
        "        bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "\n",
        "    bigram_list = [(bigram, count) for bigram, count in bigram_counts.items()]\n",
        "\n",
        "\n",
        "    for bigram, count in bigram_list:\n",
        "        found = False\n",
        "        for i, (existing_bigram, existing_count) in enumerate(bigram_char_list):\n",
        "            if existing_bigram == bigram:\n",
        "                bigram_char_list[i] = (existing_bigram, existing_count + count)\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            bigram_char_list.append((bigram, count))\n",
        "\n",
        "    bigram_list = [(bigram, count) for bigram, count in bigram_counts.items()]\n",
        "    # bigram_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return bigram_list\n"
      ],
      "metadata": {
        "id": "EeRE5MPxlzS-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in processed_words:\n",
        "   bigram_for_char(word, vyanjan,bigram_char_list) # bigram for character\n",
        "bigram_char_list_sorted=sort_list_by_frequency(bigram_char_list)\n",
        "print(bigram_char_list_sorted[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wimm_xpUn5Du",
        "outputId": "3df5d258-a8b3-4e14-c56d-7251b24aec2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ર્અ', 900967), ('અર', 856288), ('અન', 740690), ('ક્અ', 615761), ('મ્આ', 578634), ('આર', 436837), ('પ્અ', 412888), ('અમ', 405502), ('વ્આ', 403416), ('મ્અ', 393171), ('ન્અ', 374634), ('ન્એ', 374190), ('અત', 357387), ('અવ', 352507), ('સ્અ', 343310), ('ત્અ', 339485), ('ન્આ', 338880), ('છ્એ', 322605), ('વ્અ', 319748), ('ય્અ', 275049)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWaHqZpM3JuI"
      },
      "source": [
        "# **uni-gram and bi-gram frequencies of syllables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YfSnc31S3Zbs"
      },
      "outputs": [],
      "source": [
        "unigram_syllables_list=[]\n",
        "unigram_syllabless=[]\n",
        "unigram_syllables_list_sorted=[]\n",
        "### unigram syllables ####################################################################################################################\n",
        "def remove_last_two(string):\n",
        "\n",
        "  if not string:\n",
        "    return \"\"\n",
        "  return string[:-1]\n",
        "def unigram_syllables(text,swar,vyanjan,swapped_dict,unigram_syllables_list,unigram_syllabless):\n",
        "\n",
        "    i = 0\n",
        "    syllables_counts = {}\n",
        "    text = ''.join(text).replace(\" \", \"\")\n",
        "    syllable=[]\n",
        "    while i < len(text):\n",
        "          ch=\"\"\n",
        "          while i < len(text) and text[i] not in swar:\n",
        "\n",
        "              ch += text[i]\n",
        "\n",
        "              i+=1\n",
        "          if i < len(text) and len(ch)>1 and text[i] == 'અ':\n",
        "\n",
        "            ch=remove_last_two(ch)\n",
        "          elif i < len(text) and len(ch)>1 :\n",
        "            ch=remove_last_two(ch)\n",
        "            ch+=swapped_dict[text[i]]\n",
        "          elif i < len(text) :\n",
        "            ch = ch+text[i]\n",
        "          syllables_counts[ch] = syllables_counts.get(ch, 0) + 1\n",
        "          syllable.append(ch)\n",
        "          i+=1\n",
        "\n",
        "    syllable_list = [(syllable, count) for syllable, count in syllables_counts.items()]\n",
        "\n",
        "    syllable_list.sort(key=lambda x: x[1], reverse=True)\n",
        "    for syllable, count in syllable_list:\n",
        "        found = False\n",
        "        for i, (existing_syllable, existing_count) in enumerate(unigram_syllables_list):\n",
        "            if existing_syllable == syllable:\n",
        "                unigram_syllables_list[i] = (existing_syllable, existing_count + count)\n",
        "                unigram_syllabless.append(existing_syllable)\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            unigram_syllables_list.append((syllable, count))\n",
        "            unigram_syllabless.append(syllable)\n",
        "\n",
        "    # print(\" unigram syllables \")\n",
        "    # print( syllable_list)\n",
        "    # return syllable\n",
        "\n",
        "\n",
        "# Example usage\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "swapped_dict = {v: k for k, v in gujarati_dict.items()}\n",
        "spchar = '@'\n",
        "count =0\n",
        "for word in processed_words:\n",
        "\n",
        "  unigram_syllables(word,swar,vyanjan,swapped_dict,unigram_syllables_list,unigram_syllabless)\n",
        "  unigram_syllabless.append(spchar)\n",
        "\n",
        "\n",
        "\n",
        "unigram_syllables_list_sorted=sort_list_by_frequency(unigram_syllables_list)\n",
        "# print(unigram_syllabless)\n",
        "print(unigram_syllables_list_sorted[:40])\n",
        "# print(unigram_syllables_list[:20])\n",
        "print(unigram_syllabless[:20])"
      ],
      "metadata": {
        "id": "aeWdjw4_ti-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_syllables_list=[]\n",
        "bigram_syllables_list_sorted=[]\n",
        "def get_bisyllable_frequency(list_of_syllables,bigram_syllables_list):\n",
        "    biagram_syllables_list = []\n",
        "    for i in range(len(list_of_syllables) - 1):\n",
        "        if list_of_syllables[i+1] == \"@\":\n",
        "            pass\n",
        "        elif list_of_syllables[i] != \"@\":\n",
        "            pair = list_of_syllables[i] + \" \" + list_of_syllables[i+1]\n",
        "            biagram_syllables_list.append(pair)\n",
        "\n",
        "    bisyllable_frequency = {}\n",
        "    i = 0\n",
        "    while i < len(biagram_syllables_list):\n",
        "        ch = biagram_syllables_list[i]\n",
        "        bisyllable_frequency[ch] = bisyllable_frequency.get(ch, 0) + 1\n",
        "        i += 1\n",
        "\n",
        "    sort_bisyllable_list = [(bisyllable, count) for bisyllable, count in bisyllable_frequency.items()]\n",
        "    sort_bisyllable_list.sort(key=lambda x: x[1], reverse=True)\n",
        "    return sort_bisyllable_list\n",
        "\n",
        "# print(bigram_syllables_list_sorted)\n"
      ],
      "metadata": {
        "id": "BMSicu5R8Fcc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_syllables_list_sorted=get_bisyllable_frequency(unigram_syllabless,bigram_syllables_list)\n",
        "print(bigram_syllables_list_sorted[:20])"
      ],
      "metadata": {
        "id": "qTIMGoeSjie6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoodWj3oFbB5"
      },
      "source": [
        "# **Q4 install dependency**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAzFzPflFuuR"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BPE IMPLEMENT vocab size 1K**"
      ],
      "metadata": {
        "id": "rZn90DrGtSdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Train SentencePiece model\n",
        "spm.SentencePieceTrainer.train('--input=/content/drive/MyDrive/gu_100.txt --model_prefix=m_bpe --vocab_size=1000 --model_type=bpe')\n",
        "\n",
        "# Load the trained model\n",
        "sp_bpe = spm.SentencePieceProcessor()\n",
        "sp_bpe.load('m_bpe.model')\n",
        "\n",
        "# Tokenize text from 'temp.txt' file\n",
        "print('*** BPE ***')\n",
        "with open('/content/drive/MyDrive/gu_100.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "with open('/content/drive/MyDrive/ques_25.txt', 'r', encoding='utf-8') as file:\n",
        "    text2 = file.read()\n",
        "\n",
        "ques_25_bpe=sp_bpe.encode_as_pieces(text2)\n",
        "bpe_list=sp_bpe.encode_as_pieces(text)"
      ],
      "metadata": {
        "id": "fl-l7Hvco7Kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d595789b-c761-41d1-a457-f87aa22fc8ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** BPE ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "tokens_cleaned = [token.replace('▁', '') for token in bpe_list if token not in punctuation_list ]\n",
        "ques_25_cleaned_bpe = [token.replace('▁', '') for token in ques_25_bpe if token not in punctuation_list ]\n",
        "\n",
        "# Count the frequency of each token\n",
        "token_freq = Counter(tokens_cleaned)\n",
        "\n",
        "bpe_token_freq_list = list(token_freq.items())\n",
        "# print(token_freq_list)\n",
        "bpe_sorted_token_freq_list = sorted(bpe_token_freq_list, key=lambda x: x[1], reverse=True)\n",
        "print(bpe_sorted_token_freq_list[:20])\n",
        "print(ques_25_cleaned_bpe)"
      ],
      "metadata": {
        "id": "GjXk8xGuqg8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee97c7f-f26a-4251-c0e0-71ce38f0aa72"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('છે', 320962), ('ક', 245518), ('ના', 233379), ('ન', 230649), ('મ', 214983), ('ર', 208924), ('સ', 199487), ('માં', 191089), ('લ', 187060), ('ની', 172030), ('જ', 171312), ('આ', 170013), ('ને', 164128), ('પ', 156578), ('અને', 155795), ('ત', 152201), ('એ', 147101), ('વ', 146401), ('વા', 143643), ('ટ', 137690)]\n",
            "['ડિ', 'સ્', 'પ્', 'લે', 'પર', 'પણ', 'યા', 'ંત્ર', 'િક', 'ન', 'ુક', 'સ', 'ાન', 'કો', 'ર્', 'ન', 'િંગ', 'ગ', '્', 'લા', 'સ', 'થી', 'એક', 'ખાસ', 'ર', 'ક્ષ', 'ણા', 'ત્', 'મક', 'સ્', 'તર', 'છે', '4', '(', 'ઇ', '1', 'પા', 'કિ', 'સ્તા', 'નના', 'પી', 'એ', 'મ', 'ઈ', 'મ', 'રા', 'ન', 'ખ', 'ાન', 'ની', 'અધ', '્ય', 'ક્ષ', 'તા', 'માં', 'આ', 'બેઠ', 'ક', 'બો', 'લા', 'વ', 'વામાં', 'આવી', 'છે', 'આવી', 'ઘટના', 'માં', 'છે', 'તર', 'પ', 'િ', 'ં', 'ડી', 'કર', 'નાર', 'વ્યક્તિ', 'મો', 'ટે', 'ભા', 'ગે', 'પરિ', 'ચ', 'િત', 'અથવા', 'તો', 'સંબંધ', 'ી', 'હોય', 'છે', 'એ', 'વાત', 'વધ', 'ારે', 'આ', 'ઘ', 'ા', 'ત', 'જન', 'ક', 'હોય', 'છે', 'આ', 'પ', 'તા', 'વ', 'ટ', 'ની', 'આ', 'ંત', 'ર', 'મા', 'ળ', 'ખા', 'ખૂબ', 'સારી', 'રીતે', 'વિક', 'સ', 'િત', 'નથી', 'આ', 'ર', 'ક', 'મ', 'આ', 'શ', 'રે', 'છે', 'અમે', 'આ', 'મા', 'મ', 'લે', 'બહાર', 'નાં', 'વ્યક્તિ', 'નાં', 'કોઇ', 'પણ', 'દા', 'વા', 'ને', 'સંપૂર્ણ', 'રીતે', 'ફ', 'ગા', 'વી', 'એ', 'છ', 'ીએ', '', '-', 'આ', 'કિ', 'સ્', 'સા', 'ઓમાં', 'એ', 'ન્ટ', 'િ', 'બ', 'ાયો', 'ટ', 'િ', 'ક્સ', 'સાથે', 'સાર', 'વાર', 'જરૂર', 'છે', 'તમારી', 'રા', 'શિ', 'ના', 'લોકો', 'એ', 'હા', 'લ', 'દુ', 'ર્', 'ઘ', 'ટના', 'ઓ', 'થી', 'સા', 'વ', 'ધાન', 'રહે', 'વાની', 'જરૂર', 'છે', 'આ', 'બા', 'બ', 'ત', 'છે', 'ત', '્ય', 'ાર', 'બા', 'દ', 'સી', 'બી', 'આ', 'ઇ', 'અને', 'ઇ', 'ડી', 'એ', 'પ', 'ૂર', 'ક', 'ચ', 'ાર્', 'જ', 'શી', 'ટ', 'દા', 'ખ', 'લ', 'કરી', 'હતી', '.', 'ઈ', 'ન્ડ', 'િયન', 'આ', 'ઈ', 'ડ', 'લ', '1', 'ના', 'આગ', 'ામી', 'એ', 'પ', 'િ', 'સો', 'ડ', 'માં', 'ઉ', 'દ', 'િત', 'ના', 'રા', 'ય', 'ણ', 'અને', 'અલ', 'કા', 'યા', 'જ્', 'ઞ', 'િક', 'મ', 'હે', 'માન', 'બન', 'શે', 'જ્યારે', 'ક્', 'લે', 'ર', 'િસ', 'અને', 'મ', 'િત', '્સ', 'ુ', 'ઇ', 'નો', 'સ', 'માન', 'હિ', 'સ્', 'સો', 'હતો', 'આ', 'થી', '', 'ઋ', 'ષ', 'િક', 'ેશ', 'ન', 'ગ', 'રમાં', 'વિ', 'ર', 'ભ', 'દ', '્ર', 'બંધ', 'ના', 'દ્વા', 'ર', 'પાસે', 'તે', 'ફ', 'સા', 'ઈ', 'ગ', 'યો', 'હતો', 'ઉ', 'રી', 'હ', 'ુ', 'મ', 'લા', 'પછી', 'ભાર', 'તીય', 'સે', 'ના', 'તરફ', 'થી', 'સર્', 'જ', 'િક', 'લ', 'સ્ટ', '્રા', 'ઇ', 'ક', 'કરવામાં', 'આવી', 'આ', 'ખૂબ', 'જ', 'ખ', 'તર', 'ના', 'ક', 'મિ', 'શન', 'ને', 'સફ', 'ળ', 'તા', 'પૂર્', 'વ', 'ક', 'પ', 'ાર', 'પા', 'ડ', 'વાનો', 'શ્રે', 'ય', 'જાય', 'છે', 'ભારત', 'ના', 'રા', 'ષ્ટ', '્રી', 'ય', 'સુ', 'ર', 'ક્ષા', 'સ', 'લા', 'હ', 'કાર', 'અ', 'જી', 'ત', 'ડો', 'વા', 'લ', 'વ્યા', 'પ', 'ક', 'દર', 'િયા', 'કિ', 'ના', 'રો', 'જે', 'ના', 'માટે', 'સર', 'ક', 'ારે', 'ઘણી', 'બ', 'ધી', 'યો', 'જના', 'ઓ', 'પણ', 'ઘ', 'ડી', 'છે', 'કે', 'ખે', 'ડ', 'ૂ', 'તો', 'ને', 'થો', 'ડા', 'ઘણા', 'અ', 'ં', 'શે', 'પોતાની', 'ત', 'ક', 'લી', 'ફ', 'માં', 'રા', 'હ', 'ત', 'મળી', 'રહે', 'સ્', 'પે', 'ન્સ', 'રી', 'સ', 'શિ', 'ષ', '્ય', 'વ', 'ૃ', 'ત્', 'તિ', 'જો', 'તમે', 'સ', 'લ્', 'ફ', 'રમાં', 'થી', 'સામ', 'ય', 'િક', 'ટે', 'બ', 'લ', 'ને', 'ની', 'ચે', 'ખ', 'સે', 'ડો', 'તેમણે', 'કરો', 'ડો', 'પ', 'તિ', 'નો', 'દર', 'જ્', 'જો', 'પ્રા', 'પ્ત', 'કર્યો', 'છે', 'કારણ', 'કે', 'તેઓ', 'એ', 'સ', 'ત', 'ત', 'ઘણી', 'સં', 'પ', 'ત્', 'તિ', 'નિર્', 'મા', 'ણ', 'ની', 'વ્ય', 'ૂ', 'હ', 'ર', 'ચના', 'ઓ', 'નો', 'ઉપયોગ', 'કર્યો', 'છે', 'કે', 'જે', 'માંથી', 'કોઈ', 'પણ', 'ઉપયોગ', 'કરી', 'શકે', 'છે', 'આજે', 'થી', 'શરૂ', 'અહીં', 'આગ', 'ામી', 'બ', 'ારો', 'માંથી', 'મ', 'િલ', 'િય', 'ને', 'રના', 'બ', 'ાર', 'લ', 'ક્ષ', 'ણો', 'છે', 'પ', 'ટ', 'પ', 'ટ', 'તી', 'પા', 'પ', 'ણો', ':', 'લોકો', 'કા', 'દ', 'વ', 'માં', 'ન', 'હો', 'ઈ', 'શકે', 'આ', 'લે', 'ખ', 'માં', 'અમે', 'તમને', 'ક', 'હી', 'શ', 'ું', 'કે', 'સ્', 'વિ', 'મ', 'િંગ', 'ને', 'વધુ', 'ર', 'સ', 'પ્ર', 'દ', 'વધુ', 'ખાસ', 'બા', 'ંધ', 'કા', 'મ', 'મિ', 'શ', '્રણ', 'વા', 'પ', 'રી', 'ને', 'મા', 'ળ', 'ભ', 'ર', 'વા', 'એક', 'મો', 'ટી', 'સો', 'દો', 'નથી', 'માર્', 'કે', 'ટ', 'વે', 'લ', '્યુ', 'ની', 'દ', '્ર', 'ષ્ટ', 'િ', 'એ', 'કો', 'ટ', 'ક', 'મહિ', 'ન્દ', '્રા', 'બે', 'ંક', 'એ', 'ચ', 'ડી', 'એ', 'ફ', 'સી', 'બાદ', 'બીજા', 'ન', 'ં', 'બર', 'ની', '.', '.', '', 'એ', 'એ', 'ફ', 'ડી', 'ના', 'વ્યા', 'જ', 'દ', 'રમાં', '', 'ટકા', 'સુધી', 'નો']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# bigram of **Tokenizer function**"
      ],
      "metadata": {
        "id": "-QqT_L-N53MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bigramTokenizer_frequency(tokens_cleaned,bigram_token_list):\n",
        "    biagram_token_list = []\n",
        "    for i in range(len(tokens_cleaned) - 1):\n",
        "        pair = tokens_cleaned[i] + \" \" + tokens_cleaned[i+1]\n",
        "        biagram_token_list.append(pair)\n",
        "\n",
        "\n",
        "\n",
        "    bigram_token_frequency = {}\n",
        "    i = 0\n",
        "    while i < len(biagram_token_list):\n",
        "        ch = biagram_token_list[i]\n",
        "        bigram_token_frequency[ch] = bigram_token_frequency.get(ch, 0) + 1\n",
        "        i += 1\n",
        "\n",
        "    sort_bigram_token_list = [(bisyllable, count) for bisyllable, count in bigram_token_frequency.items()]\n",
        "    sort_bigram_token_list.sort(key=lambda x: x[1], reverse=True)\n",
        "    return sort_bigram_token_list"
      ],
      "metadata": {
        "id": "tyeIvTbO7R4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# biagram of **Tokenizer**"
      ],
      "metadata": {
        "id": "4zv2Cdeo-oPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_token_list=[]\n",
        "bigram_token_list_sorted=[]\n",
        "bigram_token_list_sorted=get_bigramTokenizer_frequency(tokens_cleaned,biagram_token_list)\n",
        "print(bigram_token_list_sorted[:20])"
      ],
      "metadata": {
        "id": "0hapAgQi-TDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **character**"
      ],
      "metadata": {
        "id": "FjxhTy5riJRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_words_bpe_1000 = [] # list contain all the corrected unicode\n",
        "\n",
        "\n",
        "    # Process each word and store cleaned versions\n",
        "for word in tokens_cleaned:\n",
        "  cleaned_word = correct_unicode(word)  # Explain what this function does\n",
        "  processed_words_bpe_1000.append(cleaned_word)\n",
        "        # print(f\"{word} => {cleaned_word}\")\n",
        "\n",
        "    # Increment the counter\n",
        "    # counter += 1\n"
      ],
      "metadata": {
        "id": "x5JhPtv-Zk3H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10\n",
        "result=[]\n",
        "result = processed_words_bpe_1000[:N]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MDvurFDaMme",
        "outputId": "7ba4834f-7626-476f-ec55-7af5363159d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '', ' મ્ ઈ', ' ઓ', ' ગ્ અ', ' સ્ ટ્ અ', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**unigram**"
      ],
      "metadata": {
        "id": "S8JetCCmfC4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_char_list_bpe_1000=[]\n",
        "\n",
        "unigram_char_list_bpe_1000_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "for word in processed_words_bpe_1000:\n",
        "\n",
        "  unigram_for_char(word, vyanjan,unigram_char_list_bpe_1000) # unigram for character\n",
        "\n",
        "  # print(\" unigram characters \")\n",
        "\n",
        "unigram_char_list_bpe_1000_sorted=sort_list_by_frequency(unigram_char_list_bpe_1000)\n"
      ],
      "metadata": {
        "id": "eHxR0hGvagvy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_char_list_bpe_1000_sorted[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8nuD1BEe6Wl",
        "outputId": "5ff02df7-beb8-4397-c260-6c00754836aa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('અ', 8585120), ('આ', 3371031), ('એ', 2413912), ('ર્', 2128526), ('ન્', 1734112), ('ઈ', 1505521), ('ક્', 1386555), ('મ્', 1324843), ('ત્', 1207724), ('વ્', 1149790), ('ઓ', 1149632), ('સ્', 976491), ('પ્', 867048), ('ય્', 745130), ('લ્', 684911), ('જ્', 585042), ('ઉ', 539474), ('ટ્', 520557), ('હ્', 516880), ('ઇ', 503000)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bigram**"
      ],
      "metadata": {
        "id": "iSveElDvfHv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_char_list_bpe_1000=[]\n",
        "\n",
        "bigram_char_list_bpe_1000_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "for word in processed_words_bpe_1000:\n",
        "   bigram_for_char(word, vyanjan,bigram_char_list_bpe_1000) # bigram for character\n",
        "bigram_char_list_bpe_1000_sorted=sort_list_by_frequency(bigram_char_list_bpe_1000)\n",
        "print(bigram_char_list_bpe_1000_sorted[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv0kOJ-KfFXJ",
        "outputId": "5de89edf-9942-45e7-adfa-babb71ccdc64"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ર્અ', 1083873), ('ક્અ', 714424), ('અર', 582265), ('મ્આ', 578596), ('પ્અ', 476944), ('મ્અ', 469870), ('ન્અ', 444891), ('ત્અ', 423454), ('સ્અ', 415369), ('વ્આ', 403405), ('વ્અ', 382561), ('ન્એ', 374087), ('અન', 347885), ('ન્આ', 338777), ('ય્અ', 335017), ('છ્એ', 320962), ('ટ્અ', 308164), ('ગ્અ', 275400), ('જ્અ', 263405), ('લ્અ', 259627)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **syllable**"
      ],
      "metadata": {
        "id": "b-J5mKz3iCBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_syllables_list_bpe_1000=[]\n",
        "unigram_syllabless=[]\n",
        "unigram_syllables_list_sorted_bpe_1000=[]\n",
        "swapped_dict = {v: k for k, v in gujarati_dict.items()}\n",
        "\n",
        "# print(swapped_dict)\n",
        "# resullt = [' મ્ ઈ', ' ઓ ગ્ અ સ્ ટ્ અ', ' ન્ આ']\n",
        "spchar = '@'\n",
        "count =0\n",
        "for word in processed_words_bpe_1000:\n",
        "\n",
        "  unigram_syllables(word,swar,vyanjan,swapped_dict,unigram_syllables_list_bpe_1000,unigram_syllabless)\n",
        "  unigram_syllabless.append(spchar)\n",
        "\n",
        "\n",
        "\n",
        "unigram_syllables_list_sorted_bpe_1000=sort_list_by_frequency(unigram_syllables_list_bpe_1000)\n",
        "# print(unigram_syllabless)"
      ],
      "metadata": {
        "id": "TykUTkrfiAj_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_syllables_list_sorted_bpe_1000[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiBeTXLcjast",
        "outputId": "b2db1c2a-35e5-4e71-bd2e-ee952be0386b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ર', 906016), ('ક', 705527), ('મા', 578596), ('મ', 457069), ('પ', 455178), ('ન', 444891), ('ત', 397141), ('વા', 384305), ('સ', 372966), ('ને', 369879), ('વ', 362845), ('અ', 354831), ('ના', 338777), ('આ', 331134), ('છે', 320962), ('ય', 271111), ('ગ', 270471), ('જ', 263405), ('લ', 259627), ('એ', 250862)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_syllables_list_bpe_1000=[]\n",
        "bigram_syllables_list_sorted_bpe_1000=[]\n",
        "bigram_syllables_list_sorted_bpe_1000=get_bisyllable_frequency(unigram_syllabless,bigram_syllables_list_bpe_1000)\n",
        "print(bigram_syllables_list_sorted_bpe_1000[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Iwae_wjr8N",
        "outputId": "5dc15c5a-5a28-4d58-e3fb-056e657695e5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('અ ને', 155795), ('ક ર', 113511), ('મા ટે', 83880), ('પ ર', 72564), ('એ ક', 70724), ('પ ણ', 59736), ('કા ર', 56880), ('ક રી', 56610), ('વા મા', 56142), ('ર વા', 55329), ('સા થે', 45106), ('તે મ', 42620), ('સ મ', 38367), ('હ તી', 32761), ('ઉ પ', 31354), ('ત મા', 30862), ('ન થી', 30440), ('આ પ', 28436), ('આ વે', 28185), ('ર ણ', 26629)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BPE IMPLEMENT vocab size 2K**"
      ],
      "metadata": {
        "id": "8XKGAtm4wiK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Train SentencePiece model\n",
        "spm.SentencePieceTrainer.train('--input=/content/drive/MyDrive/gu_100.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')\n",
        "\n",
        "# Load the trained model\n",
        "sp_bpe = spm.SentencePieceProcessor()\n",
        "sp_bpe.load('m_bpe.model')\n",
        "\n",
        "# Tokenize text from 'temp.txt' file\n",
        "print('*** BPE ***')\n",
        "with open('/content/drive/MyDrive/gu_100.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "bpe_list=sp_bpe.encode_as_pieces(text)"
      ],
      "metadata": {
        "id": "89zYqR6wwl0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "tokens_cleaned_vocab_2000 = [token.replace('▁', '') for token in bpe_list if token not in punctuation_list ]\n",
        "\n",
        "# Count the frequency of each token\n",
        "token_freq = Counter(tokens_cleaned_vocab_2000)\n",
        "\n",
        "bpe_token_freq_list = list(token_freq.items())\n",
        "# print(token_freq_list)\n",
        "bpe_sorted_token_freq_list = sorted(bpe_token_freq_list, key=lambda x: x[1], reverse=True)\n",
        "print(bpe_sorted_token_freq_list[:20])"
      ],
      "metadata": {
        "id": "azPT5Vhkwm0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# biagram of **Tokenizer**"
      ],
      "metadata": {
        "id": "5YA5GXsv-8z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_token_list_bpe_2000=[]\n",
        "bigram_token_list_bpe_2000_sorted=[]\n",
        "bigram_token_list_bpe_2000_sorted=get_bigramTokenizer_frequency(tokens_cleaned_vocab_2000,biagram_token_list)\n",
        "print(bigram_token_list_bpe_2000_sorted[:20])"
      ],
      "metadata": {
        "id": "YQVp6M6q-7nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **character**"
      ],
      "metadata": {
        "id": "aY7Q-uzdk5Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_words_bpe_2000 = [] # list contain all the corrected unicode\n",
        "\n",
        "\n",
        "    # Process each word and store cleaned versions\n",
        "for word in tokens_cleaned:\n",
        "  cleaned_word = correct_unicode(word)  # Explain what this function does\n",
        "  processed_words_bpe_2000.append(cleaned_word)\n",
        "        # print(f\"{word} => {cleaned_word}\")\n",
        "\n",
        "    # Increment the counter\n",
        "    # counter += 1\n"
      ],
      "metadata": {
        "id": "7DChrzTLkmxV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10\n",
        "result=[]\n",
        "result = processed_words_bpe_2000[:N]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj1-UmrekyaJ",
        "outputId": "f36aadfc-1f2c-4b48-e241-29491f521cd0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '', ' મ્ ઈ', ' ઓ', ' ગ્ અ', ' સ્ ટ્ અ', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**unigram**"
      ],
      "metadata": {
        "id": "LMzMt8a9ld2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_char_list_bpe_2000=[]\n",
        "\n",
        "unigram_char_list_bpe_2000_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "for word in processed_words_bpe_2000:\n",
        "\n",
        "  unigram_for_char(word, vyanjan,unigram_char_list_bpe_2000) # unigram for character\n",
        "\n",
        "  # print(\" unigram characters \")\n",
        "\n",
        "unigram_char_list_bpe_2000_sorted=sort_list_by_frequency(unigram_char_list_bpe_2000)"
      ],
      "metadata": {
        "id": "kg1KSXZ1k8Iy"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_char_list_bpe_2000_sorted[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7utqSL-glPlc",
        "outputId": "acc13e3c-fab7-4b08-991d-c69f6f55a70d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('અ', 8585120), ('આ', 3371031), ('એ', 2413912), ('ર્', 2128526), ('ન્', 1734112), ('ઈ', 1505521), ('ક્', 1386555), ('મ્', 1324843), ('ત્', 1207724), ('વ્', 1149790), ('ઓ', 1149632), ('સ્', 976491), ('પ્', 867048), ('ય્', 745130), ('લ્', 684911), ('જ્', 585042), ('ઉ', 539474), ('ટ્', 520557), ('હ્', 516880), ('ઇ', 503000)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bigram**"
      ],
      "metadata": {
        "id": "SRuQEFqRlbHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_char_list_bpe_2000=[]\n",
        "\n",
        "bigram_char_list_bpe_2000_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "for word in processed_words_bpe_2000:\n",
        "   bigram_for_char(word, vyanjan,bigram_char_list_bpe_2000) # bigram for character\n",
        "bigram_char_list_bpe_2000_sorted=sort_list_by_frequency(bigram_char_list_bpe_2000)\n",
        "print(bigram_char_list_bpe_2000_sorted[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4AanaHklRF6",
        "outputId": "56910227-c99f-4ba6-baee-92e1c4bfab60"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ર્અ', 1083873), ('ક્અ', 714424), ('અર', 582265), ('મ્આ', 578596), ('પ્અ', 476944), ('મ્અ', 469870), ('ન્અ', 444891), ('ત્અ', 423454), ('સ્અ', 415369), ('વ્આ', 403405), ('વ્અ', 382561), ('ન્એ', 374087), ('અન', 347885), ('ન્આ', 338777), ('ય્અ', 335017), ('છ્એ', 320962), ('ટ્અ', 308164), ('ગ્અ', 275400), ('જ્અ', 263405), ('લ્અ', 259627)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **syllable**"
      ],
      "metadata": {
        "id": "JKOBQnsTlmT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_syllables_list_bpe_1000=[]\n",
        "unigram_syllabless=[]\n",
        "unigram_syllables_list_sorted_bpe_1000=[]\n",
        "swapped_dict = {v: k for k, v in gujarati_dict.items()}\n",
        "\n",
        "# print(swapped_dict)\n",
        "# resullt = [' મ્ ઈ', ' ઓ ગ્ અ સ્ ટ્ અ', ' ન્ આ']\n",
        "spchar = '@'\n",
        "count =0\n",
        "for word in processed_words_bpe_1000:\n",
        "\n",
        "  unigram_syllables(word,swar,vyanjan,swapped_dict,unigram_syllables_list_bpe_1000,unigram_syllabless)\n",
        "  unigram_syllabless.append(spchar)\n",
        "\n",
        "\n",
        "\n",
        "unigram_syllables_list_sorted_bpe_1000=sort_list_by_frequency(unigram_syllables_list_bpe_1000)"
      ],
      "metadata": {
        "id": "fNTZKkjClh55"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_syllables_list_sorted_bpe_2000[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO5IFjLulx4k",
        "outputId": "9d5f0068-f00c-4a5a-8948-49ef84529334"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ર', 906016), ('ક', 705527), ('મા', 578596), ('મ', 457069), ('પ', 455178), ('ન', 444891), ('ત', 397141), ('વા', 384305), ('સ', 372966), ('ને', 369879), ('વ', 362845), ('અ', 354831), ('ના', 338777), ('આ', 331134), ('છે', 320962), ('ય', 271111), ('ગ', 270471), ('જ', 263405), ('લ', 259627), ('એ', 250862)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_syllables_list_bpe_1000=[]\n",
        "bigram_syllables_list_sorted_bpe_1000=[]\n",
        "bigram_syllables_list_sorted_bpe_1000=get_bisyllable_frequency(unigram_syllabless,bigram_syllables_list_bpe_1000)\n",
        "print(bigram_syllables_list_sorted_bpe_1000[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoVGh_aUl8c6",
        "outputId": "d16a4dc2-9032-4ebd-b992-3b8c84b60e03"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('અ ને', 155795), ('ક ર', 113511), ('મા ટે', 83880), ('પ ર', 72564), ('એ ક', 70724), ('પ ણ', 59736), ('કા ર', 56880), ('ક રી', 56610), ('વા મા', 56142), ('ર વા', 55329), ('સા થે', 45106), ('તે મ', 42620), ('સ મ', 38367), ('હ તી', 32761), ('ઉ પ', 31354), ('ત મા', 30862), ('ન થી', 30440), ('આ પ', 28436), ('આ વે', 28185), ('ર ણ', 26629)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrqGb9vbFt1Q"
      },
      "source": [
        "# **Unigram**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "spm.SentencePieceTrainer.train('--input=/content/drive/MyDrive/gu_100.txt --model_prefix=m_unigram --model_type=unigram')\n",
        "sp_unigram = spm.SentencePieceProcessor()\n",
        "sp_unigram.load('m_unigram.model')\n",
        "\n",
        "print('*** Unigram ***')\n",
        "with open('/content/drive/MyDrive/gu_100.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "with open('/content/drive/MyDrive/ques_25.txt', 'r', encoding='utf-8') as file:\n",
        "    text2 = file.read()\n",
        "\n",
        "ques_25_unigram=sp_unigram.encode_as_pieces(text2)\n",
        "unigram_list=sp_unigram.encode_as_pieces(text)\n",
        "# print(unigram_list)\n"
      ],
      "metadata": {
        "id": "OFqxkdpeujfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c330d8-8264-4bbf-ba3f-6a0ce3f9b8e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Unigram ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "tokens_cleaned_unigram = [token.replace('▁', '') for token in unigram_list if token not in punctuation_list and token not in matra ]\n",
        "ques_25_cleaned_unigram = [token.replace('▁', '') for token in ques_25_unigram if token not in punctuation_list ]\n",
        "\n",
        "# Count the frequency of each token\n",
        "token_freq = Counter(tokens_cleaned_unigram)\n",
        "\n",
        "unigram_token_freq_list = list(token_freq.items())\n",
        "# print(token_freq_list)\n",
        "unigram_sorted_token_freq_list = sorted(unigram_token_freq_list, key=lambda x: x[1], reverse=True)\n",
        "print(unigram_sorted_token_freq_list[:20])\n",
        "print(ques_25_cleaned_unigram)"
      ],
      "metadata": {
        "id": "cjR8h3JLvFjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f26d7a0-24f0-4802-b202-7e06eec74a68"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('છે', 317043), ('ના', 153495), ('અને', 152789), ('માં', 151459), ('ની', 146872), ('ને', 112305), ('આ', 99947), ('કે', 92217), ('માટે', 83880), ('એ', 66163), ('એક', 66111), ('જ', 62438), ('નો', 61701), ('તે', 61374), ('ન', 60366), ('પર', 56761), ('', 56070), ('પણ', 55206), ('ર', 50706), ('થી', 50541)]\n",
            "['ડિસ્પ્લે', 'પર', 'પણ', 'યાંત્રિક', 'નુકસાન', 'કો', 'ર્ન', 'િંગ', 'ગ્લાસ', 'થી', 'એક', 'ખાસ', 'રક્ષણાત્મક', 'સ્તર', 'છે', '4', '(', '20', '17', 'ઇ', '12', '70', 'પાકિસ્તાનના', 'પીએમ', 'ઈમરાન', 'ખાન', 'ની', 'અધ્યક્ષ', 'તા', 'માં', 'આ', 'બેઠક', 'બોલાવવા', 'માં', 'આવી', 'છે', 'આવી', 'ઘટના', 'માં', 'છેતરપિંડી', 'કરનાર', 'વ્યક્તિ', 'મોટે', 'ભાગે', 'પરિચિત', 'અથવા', 'તો', 'સંબંધી', 'હોય', 'છે', 'એ', 'વાત', 'વધારે', 'આઘાત', 'જનક', 'હોય', 'છે', 'આ', 'પ', 'તા', 'વ', 'ટ', 'ની', 'આંતર', 'મા', 'ળ', 'ખા', 'ખૂબ', 'સારી', 'રીતે', 'વિકસિત', 'નથી', 'આ', 'રકમ', 'આશરે', 'છે', 'અમે', 'આ', 'મામલે', 'બહાર', 'નાં', 'વ્યક્તિ', 'નાં', 'કોઇ', 'પણ', 'દાવા', 'ને', 'સંપૂર્ણ', 'રીતે', 'ફગાવી', 'એ', 'છીએ', ')', '-', 'આ', 'કિસ્સાઓમાં', 'એન્ટિ', 'બ', 'ાયો', 'ટ', 'િક્સ', 'સાથે', 'સારવાર', 'જરૂર', 'છે', 'તમારી', 'રાશિ', 'ના', 'લોકોએ', 'હાલ', 'દુર્ઘટના', 'ઓ', 'થી', 'સાવ', 'ધાન', 'રહેવાની', 'જરૂર', 'છે', 'આ', 'બાબત', 'છે', 'ત્યારબાદ', 'સીબીઆઇ', 'અને', 'ઇ', 'ડી', 'એ', 'પૂરક', 'ચાર્જ', 'શી', 'ટ', 'દાખલ', 'કરી', 'હતી', '.', 'ઈન્ડિયન', 'આ', 'ઈડ', 'લ', '11', 'ના', 'આગામી', 'એપિસોડ', 'માં', 'ઉ', 'દ', 'િત', '', 'નારાયણ', 'અને', 'અલ', 'કા', 'યા', 'જ્ઞ', 'િક', 'મહેમાન', 'બનશે', 'જ્યારે', 'ક્લે', 'ર', 'િસ', 'અને', 'મ', 'િત', '્સ', 'ુ', 'ઇ', 'નો', 'સમાન', 'હિસ્સો', 'હતો', 'આ', 'થી', '', 'ઋ', 'ષ', 'િક', 'ેશ', 'નગર', 'માં', 'વિ', 'ર', 'ભદ્ર', 'બંધ', 'ના', 'દ્વાર', 'પાસે', 'તે', 'ફસાઈ', 'ગયો', 'હતો', 'ઉ', 'રી', 'હુમલા', 'પછી', 'ભારતીય', 'સેના', 'તરફથી', 'સર્જ', 'િકલ', 'સ્ટ્રાઇક', 'કરવામાં', 'આવી', 'આ', 'ખૂબ', 'જ', 'ખતરનાક', 'મિશન', 'ને', 'સફળતા', 'પૂર્વ', 'ક', 'પાર', 'પાડવા', 'નો', 'શ્રેય', 'જાય', 'છે', 'ભારતના', 'રાષ્ટ્રીય', 'સુરક્ષા', 'સલાહકાર', 'અ', 'જીત', 'ડો', 'વા', 'લ', 'વ્યાપક', 'દરિયા', 'કિ', 'ના', 'રો', 'જેના', 'માટે', 'સરકારે', 'ઘણી', 'બધી', 'યોજનાઓ', 'પણ', 'ઘડી', 'છે', 'કે', 'ખેડૂતો', 'ને', 'થોડા', 'ઘણા', 'અંશે', 'પોતાની', 'તકલીફ', 'માં', 'રાહત', 'મળી', 'રહે', 'સ્પે', 'ન્સ', 'રી', 'સ', 'શિષ્ય', 'વૃત્તિ', 'જો', 'તમે', 'સલ્ફ', 'ર', 'માંથી', 'સામયિક', 'ટેબલ', 'ને', 'નીચે', 'ખસે', 'ડો', 'તેમણે', 'કરોડો', 'પતિ', 'નો', 'દરજ્જો', 'પ્રાપ્ત', 'કર્યો', 'છે', 'કારણ', 'કે', 'તેઓ', 'એ', 'સતત', 'ઘણી', 'સંપત્તિ', 'નિર્માણ', 'ની', 'વ્યૂહરચના', 'ઓનો', 'ઉપયોગ', 'કર્યો', 'છે', 'કે', 'જેમાંથી', 'કોઈ', 'પણ', 'ઉપયોગ', 'કરી', 'શકે', 'છે', 'આજે', 'થી', 'શરૂ', 'અહીં', 'આગામી', 'બાર', 'ો', 'માંથી', 'મિલિયન', 'ે', 'ર', 'ના', 'બાર', 'લક્ષણો', 'છે', 'પટ', 'પટ', 'તી', 'પાપ', 'ણો', ':', 'લોકો', 'કાદવ', 'માં', 'ન', 'હોઈ', 'શકે', 'આ', 'લેખમાં', 'અમે', 'તમને', 'કહી', 'શું', 'કે', 'સ્વિમિંગ', 'ને', 'વધુ', 'રસપ્રદ', 'વધુ', 'ખાસ', 'બાંધકામ', 'મિશ્રણ', 'વાપરી', 'ને', 'માળ', 'ભરવા', 'એક', 'મોટી', 'સોદો', 'નથી', 'માર્કેટ', 'વેલ', '્યુ', 'ની', 'દ્રષ્ટિએ', 'કોટ', 'ક', 'મહિન્દ્રા', 'બેંક', 'એચડી', 'એફ', 'સી', 'બાદ', 'બીજા', 'નંબર', 'ની', '.', '.', 'S', 'એ', 'એફ', 'ડી', 'ના', 'વ્યાજ', 'દર', 'માં', '0', '25', 'ટકા', 'સુધી', 'નો']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# biagram of **Tokenizer**"
      ],
      "metadata": {
        "id": "ZDnAx-P0_a7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_token_list_unigram=[]\n",
        "bigram_token_list_unigram_sorted=[]\n",
        "bigram_token_list_unigram_sorted=get_bigramTokenizer_frequency(tokens_cleaned_unigram ,bigram_token_list_unigram)\n",
        "print(bigram_token_list_unigram_sorted[:20])"
      ],
      "metadata": {
        "id": "ad5N49lz_fnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **character**"
      ],
      "metadata": {
        "id": "OuItRgCTmxRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_words_unigram = [] # list contain all the corrected unicode\n",
        "\n",
        "\n",
        "    # Process each word and store cleaned versions\n",
        "for word in tokens_cleaned_unigram:\n",
        "  cleaned_word = correct_unicode(word)  # Explain what this function does\n",
        "  processed_words_unigram.append(cleaned_word)\n",
        "        # print(f\"{word} => {cleaned_word}\")\n",
        "\n",
        "    # Increment the counter\n",
        "    # counter += 1\n"
      ],
      "metadata": {
        "id": "Q2HAkAvhmnoE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10\n",
        "result=[]\n",
        "result = processed_words_unigram[:N]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su5mNO2nm1qe",
        "outputId": "099a0ea1-54bf-4639-c78f-7b6d0002f8c6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', ' મ્ ઈ', ' ઓ ગ્ અ સ્ ટ્ અ', '', ' ન્ આ', ' ર્ ઓ જ્ અ', ' આ દ્ ઇ વ્ આ સ્ ઈ', ' વ્ ઇ ક્ આ સ્ અ', ' સ્ ગ્ અ ઠ્ અ ન્ અ', ' દ્ વ્ આ ર્ આ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**unigram**"
      ],
      "metadata": {
        "id": "vBSqf5UYnCGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_char_list_unigram=[]\n",
        "\n",
        "unigram_char_list_unigram_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "for word in processed_words_unigram:\n",
        "\n",
        "  unigram_for_char(word, vyanjan,unigram_char_list_unigram) # unigram for character\n",
        "\n",
        "  # print(\" unigram characters \")\n",
        "\n",
        "unigram_char_list_unigram_sorted=sort_list_by_frequency(unigram_char_list_unigram)"
      ],
      "metadata": {
        "id": "1Zh2DLwXm-0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_char_list_unigram_sorted[:20])"
      ],
      "metadata": {
        "id": "Oeq7E8HZnKcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bigram**"
      ],
      "metadata": {
        "id": "x3RgDj5Znmzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_char_list_unigram=[]\n",
        "\n",
        "bigram_char_list_unigram_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "for word in processed_words_unigram:\n",
        "   bigram_for_char(word, vyanjan,bigram_char_list_unigram) # bigram for character\n",
        "bigram_char_list_unigram_sorted=sort_list_by_frequency(bigram_char_list_unigram)\n",
        "print(bigram_char_list_unigram_sorted[:20])"
      ],
      "metadata": {
        "id": "R72vyimonoml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **syllable**"
      ],
      "metadata": {
        "id": "Cf7lHQgcozU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_syllables_list_unigram=[]\n",
        "unigram_syllabless=[]\n",
        "unigram_syllables_list_sorted_unigram=[]\n",
        "swapped_dict = {v: k for k, v in gujarati_dict.items()}\n",
        "\n",
        "# print(swapped_dict)\n",
        "# resullt = [' મ્ ઈ', ' ઓ ગ્ અ સ્ ટ્ અ', ' ન્ આ']\n",
        "spchar = '@'\n",
        "count =0\n",
        "for word in processed_words_unigram:\n",
        "\n",
        "  unigram_syllables(word,swar,vyanjan,swapped_dict,unigram_syllables_list_unigram,unigram_syllabless)\n",
        "  unigram_syllabless.append(spchar)\n",
        "\n",
        "\n",
        "\n",
        "unigram_syllables_list_sorted_unigram=sort_list_by_frequency(unigram_syllables_list_unigram)"
      ],
      "metadata": {
        "id": "GozsjkYQo0ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_syllables_list_sorted_unigram[:20])"
      ],
      "metadata": {
        "id": "6Xfq1a6Uqq8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_syllables_list_unigram=[]\n",
        "bigram_syllables_list_sorted_unigram=[]\n",
        "bigram_syllables_list_sorted_unigram=get_bisyllable_frequency(unigram_syllabless,bigram_syllables_list_unigram)\n",
        "print(bigram_syllables_list_sorted_unigram[:20])"
      ],
      "metadata": {
        "id": "wEtA7EXuquRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IndicBERT**"
      ],
      "metadata": {
        "id": "xWnmSzLkwU7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_n9bpppmE3_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "2Xo9uwuwwuTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert',keep_accents=True)\n",
        "model = AutoModel.from_pretrained('ai4bharat/indic-bert')\n"
      ],
      "metadata": {
        "id": "wwNMKNEayPHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print('*** IndicBert ***')\n",
        "# sentence = \"લોકો બેંકોની બહાર ઊભાં રહી ગયાં હતાં\"\n",
        "# tokenized_input = tokenizer(sentence)\n",
        "\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/gu_100.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "# Tokenize a sentence\n",
        "with open('/content/drive/MyDrive/ques_25.txt', 'r', encoding='utf-8') as file:\n",
        "    text2 = file.read()\n",
        "tokens = tokenizer(text,max_length=1000,truncation=True)\n",
        "tokens2 = tokenizer(text2)\n",
        "indicBert_list=tokenizer.convert_ids_to_tokens(tokens['input_ids'])\n",
        "indicBert_list2=tokenizer.convert_ids_to_tokens(tokens2['input_ids'])\n",
        "# print(\"segmented input sentence \",tokenizer.convert_ids_to_tokens(tokens['input_ids']))\n",
        "\n",
        "indicBert_tokens_cleaned = [token.replace('▁', '') for token in indicBert_list if token not in punctuation_list and token not in matra ]\n",
        "indicBert_q25_list_cleaned = [token.replace('▁', '') for token in indicBert_list2 if token not in punctuation_list and token not in matra ]\n",
        "\n",
        "# Count the frequency of each token\n",
        "indicBert_token_freq = Counter(indicBert_tokens_cleaned)\n",
        "\n",
        "indicBert_token_freq_list = list(indicBert_token_freq.items())\n",
        "# print(token_freq_list)\n",
        "indicBert_sorted_token_freq_list = sorted(indicBert_token_freq_list, key=lambda x: x[1], reverse=True)\n",
        "print(indicBert_sorted_token_freq_list[:20])\n",
        "print(indicBert_q25_list_cleaned)\n"
      ],
      "metadata": {
        "id": "azkGRiIDzPVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# biagram of **Tokenizer**"
      ],
      "metadata": {
        "id": "ej91xJXeyJki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_token_indicbert_1000=[]\n",
        "bigram_token_indicbert_1000_sorted=[]\n",
        "bigram_token_indicbert_1000_sorted=get_bigramTokenizer_frequency(indicBert_tokens_cleaned ,bigram_token_indicbert_1000)\n",
        "print(bigram_token_indicbert_1000_sorted[:20])"
      ],
      "metadata": {
        "id": "SkF8un2O_s50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **character**"
      ],
      "metadata": {
        "id": "KFcIqzJcyKcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_words_indicbert_1000 = [] # list contain all the corrected unicode\n",
        "\n",
        "\n",
        "    # Process each word and store cleaned versions\n",
        "for word in indicBert_tokens_cleaned:\n",
        "  cleaned_word = correct_unicode(word)  # Explain what this function does\n",
        "  processed_words_indicbert_1000.append(cleaned_word)\n",
        "        # print(f\"{word} => {cleaned_word}\")\n",
        "\n",
        "    # Increment the counter\n",
        "    # counter += 1"
      ],
      "metadata": {
        "id": "aImSIeuWyVbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10\n",
        "result=[]\n",
        "result = processed_words_indicbert_1000[:N]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "1CSo7wYhyijW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**unigram**"
      ],
      "metadata": {
        "id": "Jt-lyi0jyxXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_char_list_indicbert_1000=[]\n",
        "\n",
        "unigram_char_list_indicbert_1000_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "for word in processed_words_indicbert_1000:\n",
        "\n",
        "  unigram_for_char(word, vyanjan,unigram_char_list_indicbert_1000) # unigram for character\n",
        "\n",
        "  # print(\" unigram characters \")\n",
        "\n",
        "unigram_char_list_indicbert_1000_sorted=sort_list_by_frequency(unigram_char_list_indicbert_1000)"
      ],
      "metadata": {
        "id": "gcj6iu8_yzAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_char_list_indicbert_1000_sorted[:20])"
      ],
      "metadata": {
        "id": "IyXalO0iy6F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bigram**"
      ],
      "metadata": {
        "id": "TWmby9uoy-VF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_char_list_indicbert_1000=[]\n",
        "\n",
        "bigram_char_list_indicbert_1000_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "for word in processed_words_indicbert_1000:\n",
        "   bigram_for_char(word, vyanjan,bigram_char_list_indicbert_1000) # bigram for character\n",
        "bigram_char_list_indicbert_1000_sorted=sort_list_by_frequency(bigram_char_list_indicbert_1000)\n",
        "\n",
        "print(bigram_char_list_indicbert_1000_sorted[:20])"
      ],
      "metadata": {
        "id": "NLSanxITy9R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **syllable**"
      ],
      "metadata": {
        "id": "qxEjcHrmyNE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_syllables_list_indicbert_1000=[]\n",
        "unigram_syllabless=[]\n",
        "unigram_syllables_list_sorted_indicbert_1000=[]\n",
        "swapped_dict = {v: k for k, v in gujarati_dict.items()}\n",
        "\n",
        "# print(swapped_dict)\n",
        "# resullt = [' મ્ ઈ', ' ઓ ગ્ અ સ્ ટ્ અ', ' ન્ આ']\n",
        "spchar = '@'\n",
        "count =0\n",
        "for word in processed_words_indicbert_1000:\n",
        "\n",
        "  unigram_syllables(word,swar,vyanjan,swapped_dict,unigram_syllables_list_indicbert_1000,unigram_syllabless)\n",
        "  unigram_syllabless.append(spchar)\n",
        "\n",
        "\n",
        "\n",
        "unigram_syllables_list_sorted_indicbert_1000=sort_list_by_frequency(unigram_syllables_list_indicbert_1000)"
      ],
      "metadata": {
        "id": "NOBqM1A0zRXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_syllables_list_sorted_indicbert_1000[:20])"
      ],
      "metadata": {
        "id": "ToBD_z46z1Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_syllables_list_indicbert_1000=[]\n",
        "bigram_syllables_list_sorted_indicbert_1000=[]\n",
        "bigram_syllables_list_sorted_indicbert_1000=get_bisyllable_frequency(unigram_syllabless,bigram_syllables_list_indicbert_1000)\n",
        "print(bigram_syllables_list_sorted_indicbert_1000[:20])"
      ],
      "metadata": {
        "id": "OUF0WBsRz6cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **mBERT max_length=1000**"
      ],
      "metadata": {
        "id": "4t5V5S--1yCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',max_length=1000,do_lower_case=False)\n",
        "model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\n"
      ],
      "metadata": {
        "id": "bYi62_ia1xSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('*** mBert ***')\n",
        "with open('/content/drive/MyDrive/gu_100.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "# Tokenize a sentence\n",
        "with open('/content/drive/MyDrive/ques_25.txt', 'r', encoding='utf-8') as file:\n",
        "    text2 = file.read()\n",
        "# sentence = \"તમારું નામ શું છે?\"\n",
        "mbert_tokens = tokenizer.tokenize(text)\n",
        "mbert_ques_25_tokens = tokenizer.tokenize(text2)"
      ],
      "metadata": {
        "id": "wZudDnF7D_Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(mbert_tokens[:20])\n",
        "from collections import Counter\n",
        "mBert_tokens_cleaned = [token.replace('#', '') for token in mbert_tokens if token not in punctuation_list ]\n",
        "mbert_ques_25_tokens_cleaned = [token.replace('#', '') for token in mbert_ques_25_tokens if token not in punctuation_list]\n",
        "\n",
        "# Count the frequency of each token\n",
        "mBert_token_freq = Counter(mBert_tokens_cleaned)\n",
        "\n",
        "mBert_token_freq_list = list(mBert_token_freq.items())\n",
        "# print(token_freq_list)\n",
        "mBert_sorted_token_freq_list = sorted(mBert_token_freq_list, key=lambda x: x[1], reverse=True)\n",
        "print(mBert_sorted_token_freq_list[:20])\n",
        "print(mbert_ques_25_tokens_cleaned)"
      ],
      "metadata": {
        "id": "dsmDFDVC1tK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# biagram of **Tokenizer**"
      ],
      "metadata": {
        "id": "iNkDQcICAA2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_token_mbert_1000=[]\n",
        "bigram_token_mbert_1000_sorted=[]\n",
        "bigram_token_mbert_1000_sorted=get_bigramTokenizer_frequency(mBert_tokens_cleaned,bigram_token_mbert_1000)\n",
        "print(bigram_token_mbert_1000_sorted[:20])"
      ],
      "metadata": {
        "id": "o5a7FvQ1AEHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **character**"
      ],
      "metadata": {
        "id": "1L_gdVQVrxns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_words_mbert_1000 = [] # list contain all the corrected unicode\n",
        "\n",
        "\n",
        "    # Process each word and store cleaned versions\n",
        "for word in mBert_tokens_cleaned:\n",
        "  cleaned_word = correct_unicode(word)  # Explain what this function does\n",
        "  processed_words_mbert_1000.append(cleaned_word)\n",
        "        # print(f\"{word} => {cleaned_word}\")\n",
        "\n",
        "    # Increment the counter\n",
        "    # counter += 1"
      ],
      "metadata": {
        "id": "sjoOLFLOr4Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10\n",
        "result=[]\n",
        "result = processed_words_mbert_1000[:N]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "_nEik1LBsWVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**unigram**"
      ],
      "metadata": {
        "id": "qkTR7CJ9shVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_char_list_mbert_1000=[]\n",
        "\n",
        "unigram_char_list_mbert_1000_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "for word in processed_words_mbert_1000:\n",
        "\n",
        "  unigram_for_char(word, vyanjan,unigram_char_list_mbert_1000) # unigram for character\n",
        "\n",
        "  # print(\" unigram characters \")\n",
        "\n",
        "unigram_char_list_mbert_1000_sorted=sort_list_by_frequency(unigram_char_list_mbert_1000)"
      ],
      "metadata": {
        "id": "dLNSKMTxsi-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_char_list_mbert_1000_sorted[:20])"
      ],
      "metadata": {
        "id": "OgMuIVjKssX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bigram**"
      ],
      "metadata": {
        "id": "bm_UR5wEszpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_char_list_mbert_1000=[]\n",
        "\n",
        "bigram_char_list_mbert_1000_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "for word in processed_words_mbert_1000:\n",
        "   bigram_for_char(word, vyanjan,bigram_char_list_mbert_1000) # bigram for character\n",
        "bigram_char_list_mbert_1000_sorted=sort_list_by_frequency(bigram_char_list_mbert_1000)\n",
        "\n",
        "print(bigram_char_list_mbert_1000_sorted[:20])"
      ],
      "metadata": {
        "id": "b-ag0HCfs2Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **syllable**"
      ],
      "metadata": {
        "id": "ZdvCgGE4tBj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_syllables_list_mbert_1000=[]\n",
        "unigram_syllabless=[]\n",
        "unigram_syllables_list_sorted_mbert_1000=[]\n",
        "swapped_dict = {v: k for k, v in gujarati_dict.items()}\n",
        "\n",
        "# print(swapped_dict)\n",
        "# resullt = [' મ્ ઈ', ' ઓ ગ્ અ સ્ ટ્ અ', ' ન્ આ']\n",
        "spchar = '@'\n",
        "count =0\n",
        "for word in processed_words_mbert_1000:\n",
        "\n",
        "  unigram_syllables(word,swar,vyanjan,swapped_dict,unigram_syllables_list_mbert_1000,unigram_syllabless)\n",
        "  unigram_syllabless.append(spchar)\n",
        "\n",
        "\n",
        "\n",
        "unigram_syllables_list_sorted_mbert_1000=sort_list_by_frequency(unigram_syllables_list_mbert_1000)"
      ],
      "metadata": {
        "id": "XpdzC-tftDEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_syllables_list_sorted_mbert_1000[:20])"
      ],
      "metadata": {
        "id": "dtoS-GkttdAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_syllables_list_mbert_1000=[]\n",
        "bigram_syllables_list_sorted_mbert_1000=[]\n",
        "bigram_syllables_list_sorted_mbert_1000=get_bisyllable_frequency(unigram_syllabless,bigram_syllables_list_mbert_1000)\n",
        "print(bigram_syllables_list_sorted_mbert_1000[:20])"
      ],
      "metadata": {
        "id": "1vxcRptVtkWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **mBERT max_length=2000**"
      ],
      "metadata": {
        "id": "XeXOvO_YDB8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',max_length=2000,do_lower_case=False)\n",
        "model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")"
      ],
      "metadata": {
        "id": "9pnLtRJTFyD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('*** mBert ***')\n",
        "with open('/content/drive/MyDrive/gu_100.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "# Tokenize a sentence\n",
        "\n",
        "# sentence = \"તમારું નામ શું છે?\"\n",
        "mbert_tokens2 = tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "e2ZL9_cJDWgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "mBert_tokens_cleaned2 = [token.replace('#', '') for token in mbert_tokens if token not in punctuation_list ]\n",
        "\n",
        "# Count the frequency of each token\n",
        "mBert_token_freq2 = Counter(mBert_tokens_cleaned2)\n",
        "\n",
        "mBert_token_freq_list2 = list(mBert_token_freq2.items())\n",
        "# print(token_freq_list)\n",
        "mBert_sorted_token_freq_list2 = sorted(mBert_token_freq_list2, key=lambda x: x[1], reverse=True)\n",
        "print(mBert_sorted_token_freq_list2[:20])\n"
      ],
      "metadata": {
        "id": "mdJvUX17Dht8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# biagram of **Tokenizer**"
      ],
      "metadata": {
        "id": "U3A6rDliARE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_token_mbert_2000=[]\n",
        "bigram_token_mbert_2000_sorted=[]\n",
        "bigram_token_mbert_2000_sorted=get_bigramTokenizer_frequency(mBert_tokens_cleaned2,bigram_token_mbert_2000)\n",
        "print(bigram_token_mbert_2000_sorted[:20])"
      ],
      "metadata": {
        "id": "AgaufIE8ANCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **character**"
      ],
      "metadata": {
        "id": "Yh8Fwip8twCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_words_mbert_2000 = [] # list contain all the corrected unicode\n",
        "\n",
        "\n",
        "    # Process each word and store cleaned versions\n",
        "for word in mBert_tokens_cleaned2:\n",
        "  cleaned_word = correct_unicode(word)  # Explain what this function does\n",
        "  processed_words_mbert_2000.append(cleaned_word)\n",
        "        # print(f\"{word} => {cleaned_word}\")\n",
        "\n",
        "    # Increment the counter\n",
        "    # counter += 1"
      ],
      "metadata": {
        "id": "MhXnHhU5t9T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10\n",
        "result=[]\n",
        "result = processed_words_mbert_2000[:N]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Ih10Pz_muN40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**unigram**"
      ],
      "metadata": {
        "id": "LCTVM3AouQRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_char_list_mbert_2000=[]\n",
        "\n",
        "unigram_char_list_mbert_2000_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "for word in processed_words_mbert_2000:\n",
        "\n",
        "  unigram_for_char(word, vyanjan,unigram_char_list_mbert_2000) # unigram for character\n",
        "\n",
        "  # print(\" unigram characters \")\n",
        "\n",
        "unigram_char_list_mbert_2000_sorted=sort_list_by_frequency(unigram_char_list_mbert_2000)"
      ],
      "metadata": {
        "id": "hyh-LP2CuTYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_char_list_mbert_2000_sorted[:20])"
      ],
      "metadata": {
        "id": "toXSAP8Uud-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bigram**"
      ],
      "metadata": {
        "id": "FblM5YSeuRyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_char_list_mbert_2000=[]\n",
        "\n",
        "bigram_char_list_mbert_2000_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "for word in processed_words_mbert_2000:\n",
        "   bigram_for_char(word, vyanjan,bigram_char_list_mbert_2000) # bigram for character\n",
        "bigram_char_list_mbert_2000_sorted=sort_list_by_frequency(bigram_char_list_mbert_2000)\n",
        "print(bigram_char_list_mbert_2000_sorted[:20])"
      ],
      "metadata": {
        "id": "cpivjiT8ugFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **syllable**"
      ],
      "metadata": {
        "id": "f0qyzI7Gt0FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_syllables_list_mbert_2000=[]\n",
        "unigram_syllabless=[]\n",
        "unigram_syllables_list_sorted_mbert_2000=[]\n",
        "swapped_dict = {v: k for k, v in gujarati_dict.items()}\n",
        "\n",
        "# print(swapped_dict)\n",
        "# resullt = [' મ્ ઈ', ' ઓ ગ્ અ સ્ ટ્ અ', ' ન્ આ']\n",
        "spchar = '@'\n",
        "count =0\n",
        "for word in processed_words_mbert_2000:\n",
        "\n",
        "  unigram_syllables(word,swar,vyanjan,swapped_dict,unigram_syllables_list_mbert_2000,unigram_syllabless)\n",
        "  unigram_syllabless.append(spchar)\n",
        "\n",
        "\n",
        "\n",
        "unigram_syllables_list_sorted_mbert_2000=sort_list_by_frequency(unigram_syllables_list_mbert_2000)"
      ],
      "metadata": {
        "id": "4lSSGvdivqsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_syllables_list_sorted_mbert_2000[:20])"
      ],
      "metadata": {
        "id": "ZcWt6Cobv3fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_syllables_list_mbert_2000=[]\n",
        "bigram_syllables_list_sorted_mbert_2000=[]\n",
        "bigram_syllables_list_sorted_mbert_2000=get_bisyllable_frequency(unigram_syllabless,bigram_syllables_list_mbert_2000)\n",
        "print(bigram_syllables_list_sorted_mbert_2000[:20])"
      ],
      "metadata": {
        "id": "YrFOcNMFv8mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WhitespaceTokenizer**"
      ],
      "metadata": {
        "id": "MY89ilRMFnEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import WhitespaceTokenizer() method from nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "# Create a reference variable for Class WhitespaceTokenizer\n",
        "tk = WhitespaceTokenizer()\n",
        "\n",
        "# # Create a string input\n",
        "# gfg = \" લોકો બેંકોની બહાર ઊભાં રહી ગયાં હતાં\"\n",
        "\n",
        "# # Use tokenize method\n",
        "# geek = tk.tokenize(gfg)\n",
        "\n",
        "# print(geek)\n",
        "print('*** WhitespaceTokenizer ***')\n",
        "# sentence = \"લોકો બેંકોની બહાર ઊભાં રહી ગયાં હતાં\"\n",
        "# tokenized_input = tokenizer(sentence)\n",
        "\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/gu_100.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "# Tokenize a sentence\n",
        "with open('/content/drive/MyDrive/ques_25.txt', 'r', encoding='utf-8') as file:\n",
        "    text2 = file.read()\n",
        "whitespace_list = tk.tokenize(text)\n",
        "whitespace_list_ques_25 = tk.tokenize(text2)\n",
        "\n",
        "# print(\"segmented input sentence \",tokenizer.convert_ids_to_tokens(tokens['input_ids']))\n",
        "\n",
        "whitespace_tokens_cleaned = [token.replace('▁', '') for token in whitespace_list if token not in punctuation_list and token not in matra ]\n",
        "whitespace_tokens_cleaned_ques_25 = [token.replace('▁', '') for token in whitespace_list_ques_25 if token not in punctuation_list and token not in matra ]\n",
        "\n",
        "# Count the frequency of each token\n",
        "whitespace_token_freq = Counter(whitespace_tokens_cleaned)\n",
        "\n",
        "whitespace_token_freq_list = list(whitespace_token_freq.items())\n",
        "# print(token_freq_list)\n",
        "whitespace_sorted_token_freq_list = sorted(whitespace_token_freq_list, key=lambda x: x[1], reverse=True)\n",
        "print(whitespace_sorted_token_freq_list[:20])\n",
        "print(whitespace_tokens_cleaned_ques_25)\n",
        "\n"
      ],
      "metadata": {
        "id": "jaOGizkEFycc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# biagram of **Tokenizer**"
      ],
      "metadata": {
        "id": "URAcNEIKAfFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_token_whitespace=[]\n",
        "bigram_token_whitespace_sorted=[]\n",
        "bigram_token_whitespace_sorted=get_bigramTokenizer_frequency(whitespace_tokens_cleaned,bigram_token_whitespace)\n",
        "print(bigram_token_whitespace_sorted[:20])"
      ],
      "metadata": {
        "id": "t210M34DAVoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **character**"
      ],
      "metadata": {
        "id": "NRUdMHc5xjzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_words_whitespace = [] # list contain all the corrected unicode\n",
        "\n",
        "\n",
        "    # Process each word and store cleaned versions\n",
        "for word in whitespace_tokens_cleaned:\n",
        "  cleaned_word = correct_unicode(word)  # Explain what this function does\n",
        "  processed_words_whitespace.append(cleaned_word)\n",
        "        # print(f\"{word} => {cleaned_word}\")\n",
        "\n",
        "    # Increment the counter\n",
        "    # counter += 1\n",
        "N = 10\n",
        "result=[]\n",
        "result = processed_words_whitespace[:N]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Gepkdh6NwO28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**unigram**"
      ],
      "metadata": {
        "id": "2zdwaeZVwp7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_char_list_whitespace=[]\n",
        "\n",
        "unigram_char_list_whitespace_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "for word in processed_words_whitespace:\n",
        "\n",
        "  unigram_for_char(word, vyanjan,unigram_char_list_whitespace) # unigram for character\n",
        "\n",
        "  # print(\" unigram characters \")\n",
        "\n",
        "unigram_char_list_whitespace_sorted=sort_list_by_frequency(unigram_char_list_whitespace)\n",
        "print(unigram_char_list_whitespace_sorted[:20])"
      ],
      "metadata": {
        "id": "Wr_oSZx8wvjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bigram**"
      ],
      "metadata": {
        "id": "ZriOsvUtwrd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_char_list_whitespace=[]\n",
        "\n",
        "bigram_char_list_whitespace_sorted=[]\n",
        "\n",
        "from collections import Counter\n",
        "for word in processed_words_whitespace:\n",
        "   bigram_for_char(word, vyanjan,bigram_char_list_whitespace) # bigram for character\n",
        "bigram_char_list_whitespace_sorted=sort_list_by_frequency(bigram_char_list_whitespace)\n",
        "print(bigram_char_list_whitespace_sorted[:20])"
      ],
      "metadata": {
        "id": "Ux1UIdjTw6OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unigram and bigram of **syllable**"
      ],
      "metadata": {
        "id": "fXcM4um3xAOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_syllables_list_whitespace=[]\n",
        "unigram_syllabless=[]\n",
        "unigram_syllables_list_sorted_whitespace=[]\n",
        "swapped_dict = {v: k for k, v in gujarati_dict.items()}\n",
        "\n",
        "# print(swapped_dict)\n",
        "# resullt = [' મ્ ઈ', ' ઓ ગ્ અ સ્ ટ્ અ', ' ન્ આ']\n",
        "spchar = '@'\n",
        "count =0\n",
        "for word in processed_words_whitespace:\n",
        "\n",
        "  unigram_syllables(word,swar,vyanjan,swapped_dict,unigram_syllables_list_whitespace,unigram_syllabless)\n",
        "  unigram_syllabless.append(spchar)\n",
        "\n",
        "\n",
        "\n",
        "unigram_syllables_list_sorted_whitespace=sort_list_by_frequency(unigram_syllables_list_whitespace)\n",
        "print(unigram_syllables_list_sorted_whitespace[:20])\n"
      ],
      "metadata": {
        "id": "zAmpXh1JxERb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_syllables_list_whitespace=[]\n",
        "bigram_syllables_list_sorted_whitespace=[]\n",
        "bigram_syllables_list_sorted_whitespace=get_bisyllable_frequency(unigram_syllabless,bigram_syllables_list_whitespace)\n",
        "print(bigram_syllables_list_sorted_whitespace[:20])"
      ],
      "metadata": {
        "id": "P07jCt6jxVzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q5  find the precision, recall and F-score for the 25 sentences.**"
      ],
      "metadata": {
        "id": "9i-chccxNHXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ques_25_cleaned_bpe\n",
        "# ques_25_cleaned_unigram\n",
        "# mbert_ques_25_tokens_cleaned\n",
        "# whitespace_tokens_cleaned_ques_25\n",
        "\n",
        "def calculate_metrics(ground_truth, predicted):\n",
        "    true_positives = len(set(ground_truth) & set(predicted))\n",
        "    precision = true_positives / len(predicted) if len(predicted) > 0 else 0\n",
        "    recall = true_positives / len(ground_truth) if len(ground_truth) > 0 else 0\n",
        "    f_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F-score:\", f_score)\n",
        "\n",
        "\n",
        "# Example lists\n",
        "\n",
        "ground_truth_labels = ['ગ્લાસથી', 'એક', 'રક્ષણાત્મક સ્તર', 'છે', 'યાંત્રિક નુકસાન', 'કોર્નિંગ', 'આ', 'પાકિસ્તાનના', 'ઈમરાન ખાનની', 'બોલાવવામાં', 'આવી છે', 'પીએમ', 'છેતરપિંડી', 'હોય છે', 'આઘાતજનક', 'આ', 'સારી', 'વિકસિત', 'પતાવટની', 'ખૂબ', 'આંતરમાળખા', 'આ', 'છે', 'આશરે', 'ફગાવીએ', 'સંપૂર્ણ', 'દાવાને', 'વ્યક્તિનાં', 'અમે', 'આ', 'મામલે', 'છીએ', 'આ', 'કિસ્સાઓમાં', 'જરૂર', 'છે', 'એન્ટિબાયોટિક્સ', 'તમારી', 'રાશિના', 'લોકોએ', 'દુર્ઘટનાઓથી', 'રહેવાની', 'જરૂર', 'હાલ', 'છે', 'આ', 'છે', 'બાબત', 'ઇડીએ', 'પૂરક', 'હતી', 'ચાર્જશીટ', 'ઈન્ડિયન આઈડલ 11ના', 'એપિસોડમાં', 'મહેમાન', 'બનશે', 'મિત્સુઇનો', 'હતો', 'બંધના', 'નગરમાં', 'હતો', 'મિશનને', 'ખતરનાક', 'પાડવાનો', 'ભારતના', 'સફળતા', 'આવી', 'વ્યાપક', 'દરિયાકિનારો', 'સરકારે', 'તકલીફમાં', 'ખેડૂતોને', 'થોડા ઘણા', 'અંશે પોતાની તકલીફમાં', 'યોજનાઓ', 'રહે', 'શિષ્યવૃત્તિ', 'સલ્ફરમાંથી', 'સામયિક ટેબલને', 'પ્રાપ્ત કર્યો', 'નિર્માણની', 'વ્યૂહરચનાઓનો', 'આજેથી', 'તેમણે', 'બારોમાંથી મિલિયનેરના', 'છે', 'મિલિયનેર', 'પાપણો', 'લોકો', 'શકે', 'લેખમાં', 'અમે', 'તમને', 'કહીશું', 'સ્વિમિંગને', 'વધુ', 'રસપ્રદ', 'વાપરીને', 'એક', 'ખાસ', 'મોટી મિશ્રણ વેલ્યુની', 'બીજા નંબરની', 'એફડીના', 'સુધીનો', 'એચડીએફસી', 'SBI', 'એફડી']\n",
        "# predicted=['ડિ', 'સ્', 'પ્', 'લે', 'પર', 'પણ', 'યા', 'ંત્ર', 'િક', 'ન', 'ુક', 'સ', 'ાન', 'કો', 'ર્', 'ન', 'િંગ', 'ગ', '્', 'લા', 'સ', 'થી', 'એક', 'ખાસ', 'ર', 'ક્ષ', 'ણા', 'ત્', 'મક', 'સ્', 'તર', 'છે', '4', '(', 'ઇ', '1', 'પા', 'કિ', 'સ્તા', 'નના', 'પી', 'એ', 'મ', 'ઈ', 'મ', 'રા', 'ન', 'ખ', 'ાન', 'ની', 'અધ', '્ય', 'ક્ષ', 'તા', 'માં', 'આ', 'બેઠ', 'ક', 'બો', 'લા', 'વ', 'વામાં', 'આવી', 'છે', 'આવી', 'ઘટના', 'માં', 'છે', 'તર', 'પ', 'િ', 'ં', 'ડી', 'કર', 'નાર', 'વ્યક્તિ', 'મો', 'ટે', 'ભા', 'ગે', 'પરિ', 'ચ', 'િત', 'અથવા', 'તો', 'સંબંધ', 'ી', 'હોય', 'છે', 'એ', 'વાત', 'વધ', 'ારે', 'આ', 'ઘ', 'ા', 'ત', 'જન', 'ક', 'હોય', 'છે', 'આ', 'પ', 'તા', 'વ', 'ટ', 'ની', 'આ', 'ંત', 'ર', 'મા', 'ળ', 'ખા', 'ખૂબ', 'સારી', 'રીતે', 'વિક', 'સ', 'િત', 'નથી', 'આ', 'ર', 'ક', 'મ', 'આ', 'શ', 'રે', 'છે', 'અમે', 'આ', 'મા', 'મ', 'લે', 'બહાર', 'નાં', 'વ્યક્તિ', 'નાં', 'કોઇ', 'પણ', 'દા', 'વા', 'ને', 'સંપૂર્ણ', 'રીતે', 'ફ', 'ગા', 'વી', 'એ', 'છ', 'ીએ', '', '-', 'આ', 'કિ', 'સ્', 'સા', 'ઓમાં', 'એ', 'ન્ટ', 'િ', 'બ', 'ાયો', 'ટ', 'િ', 'ક્સ', 'સાથે', 'સાર', 'વાર', 'જરૂર', 'છે', 'તમારી', 'રા', 'શિ', 'ના', 'લોકો', 'એ', 'હા', 'લ', 'દુ', 'ર્', 'ઘ', 'ટના', 'ઓ', 'થી', 'સા', 'વ', 'ધાન', 'રહે', 'વાની', 'જરૂર', 'છે', 'આ', 'બા', 'બ', 'ત', 'છે', 'ત', '્ય', 'ાર', 'બા', 'દ', 'સી', 'બી', 'આ', 'ઇ', 'અને', 'ઇ', 'ડી', 'એ', 'પ', 'ૂર', 'ક', 'ચ', 'ાર્', 'જ', 'શી', 'ટ', 'દા', 'ખ', 'લ', 'કરી', 'હતી', '.', 'ઈ', 'ન્ડ', 'િયન', 'આ', 'ઈ', 'ડ', 'લ', '1', 'ના', 'આગ', 'ામી', 'એ', 'પ', 'િ', 'સો', 'ડ', 'માં', 'ઉ', 'દ', 'િત', 'ના', 'રા', 'ય', 'ણ', 'અને', 'અલ', 'કા', 'યા', 'જ્', 'ઞ', 'િક', 'મ', 'હે', 'માન', 'બન', 'શે', 'જ્યારે', 'ક્', 'લે', 'ર', 'િસ', 'અને', 'મ', 'િત', '્સ', 'ુ', 'ઇ', 'નો', 'સ', 'માન', 'હિ', 'સ્', 'સો', 'હતો', 'આ', 'થી', '', 'ઋ', 'ષ', 'િક', 'ેશ', 'ન', 'ગ', 'રમાં', 'વિ', 'ર', 'ભ', 'દ', '્ર', 'બંધ', 'ના', 'દ્વા', 'ર', 'પાસે', 'તે', 'ફ', 'સા', 'ઈ', 'ગ', 'યો', 'હતો', 'ઉ', 'રી', 'હ', 'ુ', 'મ', 'લા', 'પછી', 'ભાર', 'તીય', 'સે', 'ના', 'તરફ', 'થી', 'સર્', 'જ', 'િક', 'લ', 'સ્ટ', '્રા', 'ઇ', 'ક', 'કરવામાં', 'આવી', 'આ', 'ખૂબ', 'જ', 'ખ', 'તર', 'ના', 'ક', 'મિ', 'શન', 'ને', 'સફ', 'ળ', 'તા', 'પૂર્', 'વ', 'ક', 'પ', 'ાર', 'પા', 'ડ', 'વાનો', 'શ્રે', 'ય', 'જાય', 'છે', 'ભારત', 'ના', 'રા', 'ષ્ટ', '્રી', 'ય', 'સુ', 'ર', 'ક્ષા', 'સ', 'લા', 'હ', 'કાર', 'અ', 'જી', 'ત', 'ડો', 'વા', 'લ', 'વ્યા', 'પ', 'ક', 'દર', 'િયા', 'કિ', 'ના', 'રો', 'જે', 'ના', 'માટે', 'સર', 'ક', 'ારે', 'ઘણી', 'બ', 'ધી', 'યો', 'જના', 'ઓ', 'પણ', 'ઘ', 'ડી', 'છે', 'કે', 'ખે', 'ડ', 'ૂ', 'તો', 'ને', 'થો', 'ડા', 'ઘણા', 'અ', 'ં', 'શે', 'પોતાની', 'ત', 'ક', 'લી', 'ફ', 'માં', 'રા', 'હ', 'ત', 'મળી', 'રહે', 'સ્', 'પે', 'ન્સ', 'રી', 'સ', 'શિ', 'ષ', '્ય', 'વ', 'ૃ', 'ત્', 'તિ', 'જો', 'તમે', 'સ', 'લ્', 'ફ', 'રમાં', 'થી', 'સામ', 'ય', 'િક', 'ટે', 'બ', 'લ', 'ને', 'ની', 'ચે', 'ખ', 'સે', 'ડો', 'તેમણે', 'કરો', 'ડો', 'પ', 'તિ', 'નો', 'દર', 'જ્', 'જો', 'પ્રા', 'પ્ત', 'કર્યો', 'છે', 'કારણ', 'કે', 'તેઓ', 'એ', 'સ', 'ત', 'ત', 'ઘણી', 'સં', 'પ', 'ત્', 'તિ', 'નિર્', 'મા', 'ણ', 'ની', 'વ્ય', 'ૂ', 'હ', 'ર', 'ચના', 'ઓ', 'નો', 'ઉપયોગ', 'કર્યો', 'છે', 'કે', 'જે', 'માંથી', 'કોઈ', 'પણ', 'ઉપયોગ', 'કરી', 'શકે', 'છે', 'આજે', 'થી', 'શરૂ', 'અહીં', 'આગ', 'ામી', 'બ', 'ારો', 'માંથી', 'મ', 'િલ', 'િય', 'ને', 'રના', 'બ', 'ાર', 'લ', 'ક્ષ', 'ણો', 'છે', 'પ', 'ટ', 'પ', 'ટ', 'તી', 'પા', 'પ', 'ણો', ':', 'લોકો', 'કા', 'દ', 'વ', 'માં', 'ન', 'હો', 'ઈ', 'શકે', 'આ', 'લે', 'ખ', 'માં', 'અમે', 'તમને', 'ક', 'હી', 'શ', 'ું', 'કે', 'સ્', 'વિ', 'મ', 'િંગ', 'ને', 'વધુ', 'ર', 'સ', 'પ્ર', 'દ', 'વધુ', 'ખાસ', 'બા', 'ંધ', 'કા', 'મ', 'મિ', 'શ', '્રણ', 'વા', 'પ', 'રી', 'ને', 'મા', 'ળ', 'ભ', 'ર', 'વા', 'એક', 'મો', 'ટી', 'સો', 'દો', 'નથી', 'માર્', 'કે', 'ટ', 'વે', 'લ', '્યુ', 'ની', 'દ', '્ર', 'ષ્ટ', 'િ', 'એ', 'કો', 'ટ', 'ક', 'મહિ', 'ન્દ', '્રા', 'બે', 'ંક', 'એ', 'ચ', 'ડી', 'એ', 'ફ', 'સી', 'બાદ', 'બીજા', 'ન', 'ં', 'બર', 'ની', '.', '.', '', 'એ', 'એ', 'ફ', 'ડી', 'ના', 'વ્યા', 'જ', 'દ', 'રમાં', '', 'ટકા', 'સુધી', 'નો']\n",
        "print(\"Precision Recall F-Score of BPE\")\n",
        "calculate_metrics(ground_truth_labels, ques_25_cleaned_bpe)\n",
        "print(\"Precision Recall F-Score of Unigram\")\n",
        "print(\" \")\n",
        "calculate_metrics(ground_truth_labels, ques_25_cleaned_unigram)\n",
        "print(\"Precision Recall F-Score of mBert\")\n",
        "print(\" \")\n",
        "calculate_metrics(ground_truth_labels, mbert_ques_25_tokens_cleaned)\n",
        "print(\"Precision Recall F-Score of IndicBert\")\n",
        "print(\" \")\n",
        "calculate_metrics(ground_truth_labels, indicBert_q25_list_cleaned)\n",
        "print(\"Precision Recall F-Score of Whitespace Tokenizer\")\n",
        "print(\" \")\n",
        "calculate_metrics(ground_truth_labels, whitespace_tokens_cleaned_ques_25)\n",
        "\n",
        "\n",
        "# print(ground_truth_labels)"
      ],
      "metadata": {
        "id": "3gWWlgUGNyzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21b3577e-d48b-4d4d-d7fb-2078fbb278d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.5\n",
            "Recall: 0.5\n",
            "F-score: 0.5\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "11yaTik4LLcHP867YIU90eO2CYqkSSe3O",
      "authorship_tag": "ABX9TyMPg7OavdiIOILHYZHtNhMA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}